{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import merge\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D, LSTM\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные:\n",
    "* **AAPL** https://finance.yahoo.com/quote/AAPL/history?p=AAPL&.tsrc=fin-srch\n",
    "* **MSFT** https://finance.yahoo.com/quote/MSFT/history?p=MSFT&.tsrc=fin-srch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_PATH = \"/mnt/data/home/probachaydmitry/Downloads/AAPL.csv\"\n",
    "MSFT_PATH = \"/mnt/data/home/probachaydmitry/Downloads/MSFT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data = pd.read_csv(AAPL_PATH)[::-1]\n",
    "msft_data = pd.read_csv(MSFT_PATH)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>195.339996</td>\n",
       "      <td>197.690002</td>\n",
       "      <td>190.779999</td>\n",
       "      <td>191.050003</td>\n",
       "      <td>191.050003</td>\n",
       "      <td>42359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>190.020004</td>\n",
       "      <td>196.330002</td>\n",
       "      <td>189.809998</td>\n",
       "      <td>195.089996</td>\n",
       "      <td>195.089996</td>\n",
       "      <td>51034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>186.229996</td>\n",
       "      <td>189.490005</td>\n",
       "      <td>184.729996</td>\n",
       "      <td>188.160004</td>\n",
       "      <td>188.160004</td>\n",
       "      <td>31035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>188.350006</td>\n",
       "      <td>188.990005</td>\n",
       "      <td>185.919998</td>\n",
       "      <td>186.529999</td>\n",
       "      <td>186.529999</td>\n",
       "      <td>31646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>185.800003</td>\n",
       "      <td>188.389999</td>\n",
       "      <td>185.789993</td>\n",
       "      <td>188.020004</td>\n",
       "      <td>188.020004</td>\n",
       "      <td>26219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>184.850006</td>\n",
       "      <td>187.330002</td>\n",
       "      <td>183.740005</td>\n",
       "      <td>186.119995</td>\n",
       "      <td>186.119995</td>\n",
       "      <td>39042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>183.899994</td>\n",
       "      <td>184.100006</td>\n",
       "      <td>182.559998</td>\n",
       "      <td>183.729996</td>\n",
       "      <td>183.729996</td>\n",
       "      <td>23579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>182.250000</td>\n",
       "      <td>183.300003</td>\n",
       "      <td>180.919998</td>\n",
       "      <td>181.710007</td>\n",
       "      <td>181.710007</td>\n",
       "      <td>31032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>182.669998</td>\n",
       "      <td>179.369995</td>\n",
       "      <td>180.910004</td>\n",
       "      <td>180.910004</td>\n",
       "      <td>32467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>175.490005</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>175.350006</td>\n",
       "      <td>178.899994</td>\n",
       "      <td>178.899994</td>\n",
       "      <td>32011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>170.320007</td>\n",
       "      <td>173.070007</td>\n",
       "      <td>169.500000</td>\n",
       "      <td>172.910004</td>\n",
       "      <td>172.910004</td>\n",
       "      <td>23999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>173.869995</td>\n",
       "      <td>174.440002</td>\n",
       "      <td>172.020004</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>24796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>174.669998</td>\n",
       "      <td>175.490005</td>\n",
       "      <td>173.940002</td>\n",
       "      <td>174.520004</td>\n",
       "      <td>174.520004</td>\n",
       "      <td>20810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>175.940002</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>174.539993</td>\n",
       "      <td>175.529999</td>\n",
       "      <td>175.529999</td>\n",
       "      <td>19737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>175.690002</td>\n",
       "      <td>177.750000</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>175.850006</td>\n",
       "      <td>175.850006</td>\n",
       "      <td>27436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>174.279999</td>\n",
       "      <td>175.149994</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.970001</td>\n",
       "      <td>174.970001</td>\n",
       "      <td>25886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4762</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>174.320007</td>\n",
       "      <td>174.910004</td>\n",
       "      <td>172.919998</td>\n",
       "      <td>173.149994</td>\n",
       "      <td>173.149994</td>\n",
       "      <td>28215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>173.210007</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>172.729996</td>\n",
       "      <td>174.869995</td>\n",
       "      <td>174.869995</td>\n",
       "      <td>27835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>173.710007</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>173.169998</td>\n",
       "      <td>174.330002</td>\n",
       "      <td>174.330002</td>\n",
       "      <td>17070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>174.160004</td>\n",
       "      <td>175.869995</td>\n",
       "      <td>173.949997</td>\n",
       "      <td>174.229996</td>\n",
       "      <td>174.229996</td>\n",
       "      <td>21873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>171.580002</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>171.380005</td>\n",
       "      <td>172.970001</td>\n",
       "      <td>172.970001</td>\n",
       "      <td>18913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>171.800003</td>\n",
       "      <td>172.369995</td>\n",
       "      <td>170.300003</td>\n",
       "      <td>171.059998</td>\n",
       "      <td>171.059998</td>\n",
       "      <td>17249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>171.190002</td>\n",
       "      <td>173.320007</td>\n",
       "      <td>170.990005</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>26114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>169.710007</td>\n",
       "      <td>171.440002</td>\n",
       "      <td>169.490005</td>\n",
       "      <td>170.929993</td>\n",
       "      <td>170.929993</td>\n",
       "      <td>18972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>171.250000</td>\n",
       "      <td>171.699997</td>\n",
       "      <td>169.750000</td>\n",
       "      <td>170.419998</td>\n",
       "      <td>170.419998</td>\n",
       "      <td>24626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>169.710007</td>\n",
       "      <td>171.259995</td>\n",
       "      <td>169.380005</td>\n",
       "      <td>170.800003</td>\n",
       "      <td>170.800003</td>\n",
       "      <td>21835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>171.389999</td>\n",
       "      <td>172.479996</td>\n",
       "      <td>169.919998</td>\n",
       "      <td>170.179993</td>\n",
       "      <td>170.179993</td>\n",
       "      <td>22490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>170.100006</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>169.699997</td>\n",
       "      <td>170.889999</td>\n",
       "      <td>170.889999</td>\n",
       "      <td>22283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>171.050003</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>169.250000</td>\n",
       "      <td>169.429993</td>\n",
       "      <td>169.429993</td>\n",
       "      <td>20993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>168.990005</td>\n",
       "      <td>170.660004</td>\n",
       "      <td>168.419998</td>\n",
       "      <td>170.410004</td>\n",
       "      <td>170.410004</td>\n",
       "      <td>23820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000-05-04</td>\n",
       "      <td>4.111607</td>\n",
       "      <td>4.116071</td>\n",
       "      <td>3.948661</td>\n",
       "      <td>3.953125</td>\n",
       "      <td>2.635956</td>\n",
       "      <td>99878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>4.247768</td>\n",
       "      <td>4.330357</td>\n",
       "      <td>3.986607</td>\n",
       "      <td>4.109375</td>\n",
       "      <td>2.740144</td>\n",
       "      <td>122449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000-05-02</td>\n",
       "      <td>4.401786</td>\n",
       "      <td>4.508929</td>\n",
       "      <td>4.196429</td>\n",
       "      <td>4.209821</td>\n",
       "      <td>2.807121</td>\n",
       "      <td>59108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>4.459821</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>4.352679</td>\n",
       "      <td>4.439732</td>\n",
       "      <td>2.960427</td>\n",
       "      <td>56548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2000-04-28</td>\n",
       "      <td>4.540179</td>\n",
       "      <td>4.553571</td>\n",
       "      <td>4.332589</td>\n",
       "      <td>4.430804</td>\n",
       "      <td>2.954473</td>\n",
       "      <td>62395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>4.185268</td>\n",
       "      <td>4.535714</td>\n",
       "      <td>4.163504</td>\n",
       "      <td>4.526786</td>\n",
       "      <td>3.018473</td>\n",
       "      <td>81650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2000-04-26</td>\n",
       "      <td>4.522321</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>4.332589</td>\n",
       "      <td>2.888983</td>\n",
       "      <td>91728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2000-04-25</td>\n",
       "      <td>4.361607</td>\n",
       "      <td>4.598214</td>\n",
       "      <td>4.359375</td>\n",
       "      <td>4.582589</td>\n",
       "      <td>3.055684</td>\n",
       "      <td>97910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2000-04-24</td>\n",
       "      <td>4.107143</td>\n",
       "      <td>4.303571</td>\n",
       "      <td>4.098214</td>\n",
       "      <td>4.303571</td>\n",
       "      <td>2.869634</td>\n",
       "      <td>110905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000-04-20</td>\n",
       "      <td>4.417411</td>\n",
       "      <td>4.455357</td>\n",
       "      <td>4.180804</td>\n",
       "      <td>4.245536</td>\n",
       "      <td>2.830937</td>\n",
       "      <td>180530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-04-19</td>\n",
       "      <td>4.506696</td>\n",
       "      <td>4.651786</td>\n",
       "      <td>4.276786</td>\n",
       "      <td>4.325893</td>\n",
       "      <td>2.884518</td>\n",
       "      <td>130037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-04-18</td>\n",
       "      <td>4.410714</td>\n",
       "      <td>4.531250</td>\n",
       "      <td>4.263393</td>\n",
       "      <td>4.531250</td>\n",
       "      <td>3.021451</td>\n",
       "      <td>97731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-04-17</td>\n",
       "      <td>3.910714</td>\n",
       "      <td>4.426339</td>\n",
       "      <td>3.895089</td>\n",
       "      <td>4.424107</td>\n",
       "      <td>2.950008</td>\n",
       "      <td>102390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>3.904018</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>3.892857</td>\n",
       "      <td>3.995536</td>\n",
       "      <td>2.664236</td>\n",
       "      <td>166905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-04-13</td>\n",
       "      <td>3.982143</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4.064732</td>\n",
       "      <td>2.710376</td>\n",
       "      <td>132456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-04-12</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.745536</td>\n",
       "      <td>3.901786</td>\n",
       "      <td>2.601723</td>\n",
       "      <td>235284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-04-11</td>\n",
       "      <td>4.410714</td>\n",
       "      <td>4.459821</td>\n",
       "      <td>4.216518</td>\n",
       "      <td>4.265625</td>\n",
       "      <td>2.844331</td>\n",
       "      <td>135455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-04-10</td>\n",
       "      <td>4.703125</td>\n",
       "      <td>4.741071</td>\n",
       "      <td>4.455357</td>\n",
       "      <td>4.464286</td>\n",
       "      <td>2.976799</td>\n",
       "      <td>53065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-04-07</td>\n",
       "      <td>4.544643</td>\n",
       "      <td>4.709821</td>\n",
       "      <td>4.482143</td>\n",
       "      <td>4.705357</td>\n",
       "      <td>3.137547</td>\n",
       "      <td>60608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>4.665179</td>\n",
       "      <td>4.803571</td>\n",
       "      <td>4.401786</td>\n",
       "      <td>4.470982</td>\n",
       "      <td>2.981264</td>\n",
       "      <td>64906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-04-05</td>\n",
       "      <td>4.516739</td>\n",
       "      <td>4.745536</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>4.656250</td>\n",
       "      <td>3.104802</td>\n",
       "      <td>114416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>4.736607</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>4.546875</td>\n",
       "      <td>3.031870</td>\n",
       "      <td>165082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-04-03</td>\n",
       "      <td>4.839286</td>\n",
       "      <td>4.982143</td>\n",
       "      <td>4.622768</td>\n",
       "      <td>4.761161</td>\n",
       "      <td>3.174755</td>\n",
       "      <td>82140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>4.551339</td>\n",
       "      <td>4.901786</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.850446</td>\n",
       "      <td>3.234293</td>\n",
       "      <td>101158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-03-30</td>\n",
       "      <td>4.770089</td>\n",
       "      <td>4.917411</td>\n",
       "      <td>4.479911</td>\n",
       "      <td>4.491071</td>\n",
       "      <td>2.994658</td>\n",
       "      <td>103600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>4.977679</td>\n",
       "      <td>4.979911</td>\n",
       "      <td>4.779575</td>\n",
       "      <td>4.854911</td>\n",
       "      <td>3.237269</td>\n",
       "      <td>59959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>4.901786</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>4.897321</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>3.313178</td>\n",
       "      <td>50741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-27</td>\n",
       "      <td>4.915179</td>\n",
       "      <td>5.169643</td>\n",
       "      <td>4.888393</td>\n",
       "      <td>4.984375</td>\n",
       "      <td>3.323597</td>\n",
       "      <td>69795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>5.087054</td>\n",
       "      <td>5.140625</td>\n",
       "      <td>4.839286</td>\n",
       "      <td>4.953125</td>\n",
       "      <td>3.302759</td>\n",
       "      <td>111728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-03-23</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>5.370536</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.046875</td>\n",
       "      <td>3.365272</td>\n",
       "      <td>140641200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4779 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "4778  2019-03-22  195.339996  197.690002  190.779999  191.050003  191.050003   \n",
       "4777  2019-03-21  190.020004  196.330002  189.809998  195.089996  195.089996   \n",
       "4776  2019-03-20  186.229996  189.490005  184.729996  188.160004  188.160004   \n",
       "4775  2019-03-19  188.350006  188.990005  185.919998  186.529999  186.529999   \n",
       "4774  2019-03-18  185.800003  188.389999  185.789993  188.020004  188.020004   \n",
       "4773  2019-03-15  184.850006  187.330002  183.740005  186.119995  186.119995   \n",
       "4772  2019-03-14  183.899994  184.100006  182.559998  183.729996  183.729996   \n",
       "4771  2019-03-13  182.250000  183.300003  180.919998  181.710007  181.710007   \n",
       "4770  2019-03-12  180.000000  182.669998  179.369995  180.910004  180.910004   \n",
       "4769  2019-03-11  175.490005  179.119995  175.350006  178.899994  178.899994   \n",
       "4768  2019-03-08  170.320007  173.070007  169.500000  172.910004  172.910004   \n",
       "4767  2019-03-07  173.869995  174.440002  172.020004  172.500000  172.500000   \n",
       "4766  2019-03-06  174.669998  175.490005  173.940002  174.520004  174.520004   \n",
       "4765  2019-03-05  175.940002  176.000000  174.539993  175.529999  175.529999   \n",
       "4764  2019-03-04  175.690002  177.750000  173.970001  175.850006  175.850006   \n",
       "4763  2019-03-01  174.279999  175.149994  172.889999  174.970001  174.970001   \n",
       "4762  2019-02-28  174.320007  174.910004  172.919998  173.149994  173.149994   \n",
       "4761  2019-02-27  173.210007  175.000000  172.729996  174.869995  174.869995   \n",
       "4760  2019-02-26  173.710007  175.300003  173.169998  174.330002  174.330002   \n",
       "4759  2019-02-25  174.160004  175.869995  173.949997  174.229996  174.229996   \n",
       "4758  2019-02-22  171.580002  173.000000  171.380005  172.970001  172.970001   \n",
       "4757  2019-02-21  171.800003  172.369995  170.300003  171.059998  171.059998   \n",
       "4756  2019-02-20  171.190002  173.320007  170.990005  172.029999  172.029999   \n",
       "4755  2019-02-19  169.710007  171.440002  169.490005  170.929993  170.929993   \n",
       "4754  2019-02-15  171.250000  171.699997  169.750000  170.419998  170.419998   \n",
       "4753  2019-02-14  169.710007  171.259995  169.380005  170.800003  170.800003   \n",
       "4752  2019-02-13  171.389999  172.479996  169.919998  170.179993  170.179993   \n",
       "4751  2019-02-12  170.100006  171.000000  169.699997  170.889999  170.889999   \n",
       "4750  2019-02-11  171.050003  171.210007  169.250000  169.429993  169.429993   \n",
       "4749  2019-02-08  168.990005  170.660004  168.419998  170.410004  170.410004   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "29    2000-05-04    4.111607    4.116071    3.948661    3.953125    2.635956   \n",
       "28    2000-05-03    4.247768    4.330357    3.986607    4.109375    2.740144   \n",
       "27    2000-05-02    4.401786    4.508929    4.196429    4.209821    2.807121   \n",
       "26    2000-05-01    4.459821    4.468750    4.352679    4.439732    2.960427   \n",
       "25    2000-04-28    4.540179    4.553571    4.332589    4.430804    2.954473   \n",
       "24    2000-04-27    4.185268    4.535714    4.163504    4.526786    3.018473   \n",
       "23    2000-04-26    4.522321    4.571429    4.285714    4.332589    2.888983   \n",
       "22    2000-04-25    4.361607    4.598214    4.359375    4.582589    3.055684   \n",
       "21    2000-04-24    4.107143    4.303571    4.098214    4.303571    2.869634   \n",
       "20    2000-04-20    4.417411    4.455357    4.180804    4.245536    2.830937   \n",
       "19    2000-04-19    4.506696    4.651786    4.276786    4.325893    2.884518   \n",
       "18    2000-04-18    4.410714    4.531250    4.263393    4.531250    3.021451   \n",
       "17    2000-04-17    3.910714    4.426339    3.895089    4.424107    2.950008   \n",
       "16    2000-04-14    3.904018    4.214286    3.892857    3.995536    2.664236   \n",
       "15    2000-04-13    3.982143    4.285714    3.875000    4.064732    2.710376   \n",
       "14    2000-04-12    4.250000    4.250000    3.745536    3.901786    2.601723   \n",
       "13    2000-04-11    4.410714    4.459821    4.216518    4.265625    2.844331   \n",
       "12    2000-04-10    4.703125    4.741071    4.455357    4.464286    2.976799   \n",
       "11    2000-04-07    4.544643    4.709821    4.482143    4.705357    3.137547   \n",
       "10    2000-04-06    4.665179    4.803571    4.401786    4.470982    2.981264   \n",
       "9     2000-04-05    4.516739    4.745536    4.428571    4.656250    3.104802   \n",
       "8     2000-04-04    4.736607    4.750000    4.169643    4.546875    3.031870   \n",
       "7     2000-04-03    4.839286    4.982143    4.622768    4.761161    3.174755   \n",
       "6     2000-03-31    4.551339    4.901786    4.500000    4.850446    3.234293   \n",
       "5     2000-03-30    4.770089    4.917411    4.479911    4.491071    2.994658   \n",
       "4     2000-03-29    4.977679    4.979911    4.779575    4.854911    3.237269   \n",
       "3     2000-03-28    4.901786    5.071429    4.897321    4.968750    3.313178   \n",
       "2     2000-03-27    4.915179    5.169643    4.888393    4.984375    3.323597   \n",
       "1     2000-03-24    5.087054    5.140625    4.839286    4.953125    3.302759   \n",
       "0     2000-03-23    5.071429    5.370536    5.000000    5.046875    3.365272   \n",
       "\n",
       "         Volume  \n",
       "4778   42359300  \n",
       "4777   51034200  \n",
       "4776   31035200  \n",
       "4775   31646400  \n",
       "4774   26219800  \n",
       "4773   39042900  \n",
       "4772   23579500  \n",
       "4771   31032500  \n",
       "4770   32467600  \n",
       "4769   32011000  \n",
       "4768   23999400  \n",
       "4767   24796400  \n",
       "4766   20810400  \n",
       "4765   19737400  \n",
       "4764   27436200  \n",
       "4763   25886200  \n",
       "4762   28215400  \n",
       "4761   27835400  \n",
       "4760   17070200  \n",
       "4759   21873400  \n",
       "4758   18913200  \n",
       "4757   17249700  \n",
       "4756   26114400  \n",
       "4755   18972800  \n",
       "4754   24626800  \n",
       "4753   21835700  \n",
       "4752   22490200  \n",
       "4751   22283500  \n",
       "4750   20993400  \n",
       "4749   23820000  \n",
       "...         ...  \n",
       "29     99878800  \n",
       "28    122449600  \n",
       "27     59108000  \n",
       "26     56548800  \n",
       "25     62395200  \n",
       "24     81650800  \n",
       "23     91728000  \n",
       "22     97910400  \n",
       "21    110905200  \n",
       "20    180530000  \n",
       "19    130037600  \n",
       "18     97731200  \n",
       "17    102390400  \n",
       "16    166905200  \n",
       "15    132456800  \n",
       "14    235284000  \n",
       "13    135455600  \n",
       "12     53065600  \n",
       "11     60608800  \n",
       "10     64906800  \n",
       "9     114416400  \n",
       "8     165082400  \n",
       "7      82140800  \n",
       "6     101158400  \n",
       "5     103600000  \n",
       "4      59959200  \n",
       "3      50741600  \n",
       "2      69795600  \n",
       "1     111728400  \n",
       "0     140641200  \n",
       "\n",
       "[4779 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>119.500000</td>\n",
       "      <td>119.589996</td>\n",
       "      <td>117.040001</td>\n",
       "      <td>117.050003</td>\n",
       "      <td>117.050003</td>\n",
       "      <td>33619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>117.139999</td>\n",
       "      <td>120.820000</td>\n",
       "      <td>117.089996</td>\n",
       "      <td>120.220001</td>\n",
       "      <td>120.220001</td>\n",
       "      <td>29854400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>117.389999</td>\n",
       "      <td>118.750000</td>\n",
       "      <td>116.709999</td>\n",
       "      <td>117.519997</td>\n",
       "      <td>117.519997</td>\n",
       "      <td>28113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>118.089996</td>\n",
       "      <td>118.440002</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>117.650002</td>\n",
       "      <td>117.650002</td>\n",
       "      <td>37588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>116.169998</td>\n",
       "      <td>117.610001</td>\n",
       "      <td>116.050003</td>\n",
       "      <td>117.570000</td>\n",
       "      <td>117.570000</td>\n",
       "      <td>31207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>115.339996</td>\n",
       "      <td>117.250000</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>115.910004</td>\n",
       "      <td>115.910004</td>\n",
       "      <td>54681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>114.540001</td>\n",
       "      <td>115.199997</td>\n",
       "      <td>114.330002</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>30763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>114.129997</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>113.779999</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>35513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>112.820000</td>\n",
       "      <td>113.989998</td>\n",
       "      <td>112.650002</td>\n",
       "      <td>113.620003</td>\n",
       "      <td>113.620003</td>\n",
       "      <td>26132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>110.989998</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>110.980003</td>\n",
       "      <td>112.830002</td>\n",
       "      <td>112.830002</td>\n",
       "      <td>26491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>109.160004</td>\n",
       "      <td>110.709999</td>\n",
       "      <td>108.800003</td>\n",
       "      <td>110.510002</td>\n",
       "      <td>110.510002</td>\n",
       "      <td>22818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>111.550003</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>110.389999</td>\n",
       "      <td>110.389999</td>\n",
       "      <td>25339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>111.870003</td>\n",
       "      <td>112.660004</td>\n",
       "      <td>111.430000</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>17687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>112.250000</td>\n",
       "      <td>112.389999</td>\n",
       "      <td>111.230003</td>\n",
       "      <td>111.699997</td>\n",
       "      <td>111.699997</td>\n",
       "      <td>19538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>113.019997</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>110.800003</td>\n",
       "      <td>112.260002</td>\n",
       "      <td>112.260002</td>\n",
       "      <td>26608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>112.889999</td>\n",
       "      <td>113.019997</td>\n",
       "      <td>111.669998</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>23501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4762</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>112.040001</td>\n",
       "      <td>112.879997</td>\n",
       "      <td>111.730003</td>\n",
       "      <td>112.029999</td>\n",
       "      <td>112.029999</td>\n",
       "      <td>29083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>111.690002</td>\n",
       "      <td>112.360001</td>\n",
       "      <td>110.879997</td>\n",
       "      <td>112.169998</td>\n",
       "      <td>112.169998</td>\n",
       "      <td>21487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>111.260002</td>\n",
       "      <td>113.239998</td>\n",
       "      <td>111.169998</td>\n",
       "      <td>112.360001</td>\n",
       "      <td>112.360001</td>\n",
       "      <td>21536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>111.760002</td>\n",
       "      <td>112.180000</td>\n",
       "      <td>111.260002</td>\n",
       "      <td>111.589996</td>\n",
       "      <td>111.589996</td>\n",
       "      <td>23750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>111.199997</td>\n",
       "      <td>109.820000</td>\n",
       "      <td>110.970001</td>\n",
       "      <td>110.970001</td>\n",
       "      <td>27763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>106.900002</td>\n",
       "      <td>109.480003</td>\n",
       "      <td>106.870003</td>\n",
       "      <td>109.410004</td>\n",
       "      <td>109.410004</td>\n",
       "      <td>29063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>107.860001</td>\n",
       "      <td>107.940002</td>\n",
       "      <td>106.290001</td>\n",
       "      <td>107.150002</td>\n",
       "      <td>107.150002</td>\n",
       "      <td>21607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>107.790001</td>\n",
       "      <td>108.660004</td>\n",
       "      <td>107.779999</td>\n",
       "      <td>108.169998</td>\n",
       "      <td>107.709999</td>\n",
       "      <td>18038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>107.910004</td>\n",
       "      <td>108.300003</td>\n",
       "      <td>107.360001</td>\n",
       "      <td>108.220001</td>\n",
       "      <td>107.759789</td>\n",
       "      <td>26606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>106.309998</td>\n",
       "      <td>107.290001</td>\n",
       "      <td>105.660004</td>\n",
       "      <td>106.900002</td>\n",
       "      <td>106.445404</td>\n",
       "      <td>21784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>107.779999</td>\n",
       "      <td>106.709999</td>\n",
       "      <td>106.809998</td>\n",
       "      <td>106.355782</td>\n",
       "      <td>18394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>106.139999</td>\n",
       "      <td>107.139999</td>\n",
       "      <td>105.480003</td>\n",
       "      <td>106.889999</td>\n",
       "      <td>106.435448</td>\n",
       "      <td>25056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>106.199997</td>\n",
       "      <td>106.580002</td>\n",
       "      <td>104.970001</td>\n",
       "      <td>105.250000</td>\n",
       "      <td>104.802422</td>\n",
       "      <td>18914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>104.389999</td>\n",
       "      <td>105.779999</td>\n",
       "      <td>104.260002</td>\n",
       "      <td>105.669998</td>\n",
       "      <td>105.220634</td>\n",
       "      <td>21461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000-05-04</td>\n",
       "      <td>35.156250</td>\n",
       "      <td>35.625000</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>35.218750</td>\n",
       "      <td>25.449842</td>\n",
       "      <td>43317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>35.187500</td>\n",
       "      <td>35.406250</td>\n",
       "      <td>34.406250</td>\n",
       "      <td>35.281250</td>\n",
       "      <td>25.495012</td>\n",
       "      <td>55354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000-05-02</td>\n",
       "      <td>36.406250</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>34.937500</td>\n",
       "      <td>25.246618</td>\n",
       "      <td>97716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>36.437500</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>35.843750</td>\n",
       "      <td>36.718750</td>\n",
       "      <td>26.533783</td>\n",
       "      <td>107811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2000-04-28</td>\n",
       "      <td>35.375000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>34.125000</td>\n",
       "      <td>34.875000</td>\n",
       "      <td>25.201454</td>\n",
       "      <td>78082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>33.718750</td>\n",
       "      <td>34.968750</td>\n",
       "      <td>33.687500</td>\n",
       "      <td>34.906250</td>\n",
       "      <td>25.224031</td>\n",
       "      <td>77669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2000-04-26</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.562500</td>\n",
       "      <td>33.687500</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>24.569155</td>\n",
       "      <td>107091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2000-04-25</td>\n",
       "      <td>34.375000</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>33.812500</td>\n",
       "      <td>34.687500</td>\n",
       "      <td>25.065952</td>\n",
       "      <td>159517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2000-04-24</td>\n",
       "      <td>33.625000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>33.312500</td>\n",
       "      <td>24.072346</td>\n",
       "      <td>313645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000-04-20</td>\n",
       "      <td>39.312500</td>\n",
       "      <td>39.937500</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>39.468750</td>\n",
       "      <td>28.520998</td>\n",
       "      <td>52387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-04-19</td>\n",
       "      <td>40.718750</td>\n",
       "      <td>40.750000</td>\n",
       "      <td>39.062500</td>\n",
       "      <td>39.343750</td>\n",
       "      <td>28.430660</td>\n",
       "      <td>53715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-04-18</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>40.968750</td>\n",
       "      <td>37.937500</td>\n",
       "      <td>40.281250</td>\n",
       "      <td>29.108118</td>\n",
       "      <td>91794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-04-17</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>37.937500</td>\n",
       "      <td>27.414469</td>\n",
       "      <td>119772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>39.562500</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>37.062500</td>\n",
       "      <td>26.782173</td>\n",
       "      <td>151217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-04-13</td>\n",
       "      <td>40.437500</td>\n",
       "      <td>41.125000</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>39.625000</td>\n",
       "      <td>28.633890</td>\n",
       "      <td>94316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-04-12</td>\n",
       "      <td>41.062500</td>\n",
       "      <td>41.125000</td>\n",
       "      <td>39.375000</td>\n",
       "      <td>39.687500</td>\n",
       "      <td>28.679054</td>\n",
       "      <td>153003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-04-11</td>\n",
       "      <td>42.562500</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>41.750000</td>\n",
       "      <td>41.937500</td>\n",
       "      <td>30.304972</td>\n",
       "      <td>71961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-04-10</td>\n",
       "      <td>44.312500</td>\n",
       "      <td>44.312500</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>31.095318</td>\n",
       "      <td>60685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-04-07</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>44.687500</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>44.531250</td>\n",
       "      <td>32.179268</td>\n",
       "      <td>82613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>42.632801</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>31.072748</td>\n",
       "      <td>66421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-04-05</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>42.937500</td>\n",
       "      <td>43.187500</td>\n",
       "      <td>31.208242</td>\n",
       "      <td>82887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>45.781250</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>42.468750</td>\n",
       "      <td>44.281250</td>\n",
       "      <td>31.998610</td>\n",
       "      <td>181244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-04-03</td>\n",
       "      <td>47.218750</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.437500</td>\n",
       "      <td>32.834122</td>\n",
       "      <td>260118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>54.125000</td>\n",
       "      <td>52.062500</td>\n",
       "      <td>53.125000</td>\n",
       "      <td>38.389305</td>\n",
       "      <td>64281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-03-30</td>\n",
       "      <td>53.093750</td>\n",
       "      <td>54.312500</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>51.687500</td>\n",
       "      <td>37.350536</td>\n",
       "      <td>64178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>52.593750</td>\n",
       "      <td>54.468750</td>\n",
       "      <td>52.562500</td>\n",
       "      <td>53.593750</td>\n",
       "      <td>38.728024</td>\n",
       "      <td>64363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>51.812500</td>\n",
       "      <td>53.718750</td>\n",
       "      <td>51.187500</td>\n",
       "      <td>52.156250</td>\n",
       "      <td>37.689251</td>\n",
       "      <td>81114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-27</td>\n",
       "      <td>53.882801</td>\n",
       "      <td>54.125000</td>\n",
       "      <td>51.968750</td>\n",
       "      <td>52.031250</td>\n",
       "      <td>37.598930</td>\n",
       "      <td>111434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>56.312500</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>54.781250</td>\n",
       "      <td>55.843750</td>\n",
       "      <td>40.353931</td>\n",
       "      <td>112196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-03-23</td>\n",
       "      <td>53.406250</td>\n",
       "      <td>56.437500</td>\n",
       "      <td>53.312500</td>\n",
       "      <td>55.937500</td>\n",
       "      <td>40.421658</td>\n",
       "      <td>148224000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4779 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "4778  2019-03-22  119.500000  119.589996  117.040001  117.050003  117.050003   \n",
       "4777  2019-03-21  117.139999  120.820000  117.089996  120.220001  120.220001   \n",
       "4776  2019-03-20  117.389999  118.750000  116.709999  117.519997  117.519997   \n",
       "4775  2019-03-19  118.089996  118.440002  116.989998  117.650002  117.650002   \n",
       "4774  2019-03-18  116.169998  117.610001  116.050003  117.570000  117.570000   \n",
       "4773  2019-03-15  115.339996  117.250000  114.589996  115.910004  115.910004   \n",
       "4772  2019-03-14  114.540001  115.199997  114.330002  114.589996  114.589996   \n",
       "4771  2019-03-13  114.129997  115.000000  113.779999  114.500000  114.500000   \n",
       "4770  2019-03-12  112.820000  113.989998  112.650002  113.620003  113.620003   \n",
       "4769  2019-03-11  110.989998  112.949997  110.980003  112.830002  112.830002   \n",
       "4768  2019-03-08  109.160004  110.709999  108.800003  110.510002  110.510002   \n",
       "4767  2019-03-07  111.400002  111.550003  109.870003  110.389999  110.389999   \n",
       "4766  2019-03-06  111.870003  112.660004  111.430000  111.750000  111.750000   \n",
       "4765  2019-03-05  112.250000  112.389999  111.230003  111.699997  111.699997   \n",
       "4764  2019-03-04  113.019997  113.250000  110.800003  112.260002  112.260002   \n",
       "4763  2019-03-01  112.889999  113.019997  111.669998  112.529999  112.529999   \n",
       "4762  2019-02-28  112.040001  112.879997  111.730003  112.029999  112.029999   \n",
       "4761  2019-02-27  111.690002  112.360001  110.879997  112.169998  112.169998   \n",
       "4760  2019-02-26  111.260002  113.239998  111.169998  112.360001  112.360001   \n",
       "4759  2019-02-25  111.760002  112.180000  111.260002  111.589996  111.589996   \n",
       "4758  2019-02-22  110.050003  111.199997  109.820000  110.970001  110.970001   \n",
       "4757  2019-02-21  106.900002  109.480003  106.870003  109.410004  109.410004   \n",
       "4756  2019-02-20  107.860001  107.940002  106.290001  107.150002  107.150002   \n",
       "4755  2019-02-19  107.790001  108.660004  107.779999  108.169998  107.709999   \n",
       "4754  2019-02-15  107.910004  108.300003  107.360001  108.220001  107.759789   \n",
       "4753  2019-02-14  106.309998  107.290001  105.660004  106.900002  106.445404   \n",
       "4752  2019-02-13  107.500000  107.779999  106.709999  106.809998  106.355782   \n",
       "4751  2019-02-12  106.139999  107.139999  105.480003  106.889999  106.435448   \n",
       "4750  2019-02-11  106.199997  106.580002  104.970001  105.250000  104.802422   \n",
       "4749  2019-02-08  104.389999  105.779999  104.260002  105.669998  105.220634   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "29    2000-05-04   35.156250   35.625000   34.656250   35.218750   25.449842   \n",
       "28    2000-05-03   35.187500   35.406250   34.406250   35.281250   25.495012   \n",
       "27    2000-05-02   36.406250   36.750000   34.750000   34.937500   25.246618   \n",
       "26    2000-05-01   36.437500   37.000000   35.843750   36.718750   26.533783   \n",
       "25    2000-04-28   35.375000   35.500000   34.125000   34.875000   25.201454   \n",
       "24    2000-04-27   33.718750   34.968750   33.687500   34.906250   25.224031   \n",
       "23    2000-04-26   35.000000   35.562500   33.687500   34.000000   24.569155   \n",
       "22    2000-04-25   34.375000   34.750000   33.812500   34.687500   25.065952   \n",
       "21    2000-04-24   33.625000   34.000000   32.500000   33.312500   24.072346   \n",
       "20    2000-04-20   39.312500   39.937500   38.750000   39.468750   28.520998   \n",
       "19    2000-04-19   40.718750   40.750000   39.062500   39.343750   28.430660   \n",
       "18    2000-04-18   38.250000   40.968750   37.937500   40.281250   29.108118   \n",
       "17    2000-04-17   37.125000   38.000000   36.500000   37.937500   27.414469   \n",
       "16    2000-04-14   39.562500   39.750000   36.625000   37.062500   26.782173   \n",
       "15    2000-04-13   40.437500   41.125000   39.500000   39.625000   28.633890   \n",
       "14    2000-04-12   41.062500   41.125000   39.375000   39.687500   28.679054   \n",
       "13    2000-04-11   42.562500   43.031250   41.750000   41.937500   30.304972   \n",
       "12    2000-04-10   44.312500   44.312500   43.000000   43.031250   31.095318   \n",
       "11    2000-04-07   43.500000   44.687500   42.500000   44.531250   32.179268   \n",
       "10    2000-04-06   43.937500   44.000000   42.632801   43.000000   31.072748   \n",
       "9     2000-04-05   44.125000   44.250000   42.937500   43.187500   31.208242   \n",
       "8     2000-04-04   45.781250   46.000000   42.468750   44.281250   31.998610   \n",
       "7     2000-04-03   47.218750   48.250000   45.000000   45.437500   32.834122   \n",
       "6     2000-03-31   53.000000   54.125000   52.062500   53.125000   38.389305   \n",
       "5     2000-03-30   53.093750   54.312500   51.250000   51.687500   37.350536   \n",
       "4     2000-03-29   52.593750   54.468750   52.562500   53.593750   38.728024   \n",
       "3     2000-03-28   51.812500   53.718750   51.187500   52.156250   37.689251   \n",
       "2     2000-03-27   53.882801   54.125000   51.968750   52.031250   37.598930   \n",
       "1     2000-03-24   56.312500   57.500000   54.781250   55.843750   40.353931   \n",
       "0     2000-03-23   53.406250   56.437500   53.312500   55.937500   40.421658   \n",
       "\n",
       "         Volume  \n",
       "4778   33619100  \n",
       "4777   29854400  \n",
       "4776   28113300  \n",
       "4775   37588700  \n",
       "4774   31207600  \n",
       "4773   54681100  \n",
       "4772   30763400  \n",
       "4771   35513800  \n",
       "4770   26132700  \n",
       "4769   26491600  \n",
       "4768   22818400  \n",
       "4767   25339000  \n",
       "4766   17687000  \n",
       "4765   19538300  \n",
       "4764   26608000  \n",
       "4763   23501200  \n",
       "4762   29083900  \n",
       "4761   21487100  \n",
       "4760   21536700  \n",
       "4759   23750600  \n",
       "4758   27763200  \n",
       "4757   29063200  \n",
       "4756   21607700  \n",
       "4755   18038500  \n",
       "4754   26606900  \n",
       "4753   21784700  \n",
       "4752   18394900  \n",
       "4751   25056600  \n",
       "4750   18914100  \n",
       "4749   21461100  \n",
       "...         ...  \n",
       "29     43317200  \n",
       "28     55354800  \n",
       "27     97716200  \n",
       "26    107811000  \n",
       "25     78082600  \n",
       "24     77669800  \n",
       "23    107091200  \n",
       "22    159517400  \n",
       "21    313645800  \n",
       "20     52387400  \n",
       "19     53715400  \n",
       "18     91794600  \n",
       "17    119772200  \n",
       "16    151217800  \n",
       "15     94316200  \n",
       "14    153003800  \n",
       "13     71961800  \n",
       "12     60685400  \n",
       "11     82613600  \n",
       "10     66421400  \n",
       "9      82887600  \n",
       "8     181244400  \n",
       "7     260118200  \n",
       "6      64281400  \n",
       "5      64178400  \n",
       "4      64363800  \n",
       "3      81114400  \n",
       "2     111434000  \n",
       "1     112196800  \n",
       "0     148224000  \n",
       "\n",
       "[4779 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plt_data(data):\n",
    "    \"\"\"Строит pyplot график.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas.core.frame.DataFrame\n",
    "               Данные в формате сайта finance.yahoo.com\n",
    "    \"\"\"\n",
    "    close_price = data.loc[:, 'Adj Close'].tolist()\n",
    "    plt.plot(close_price)\n",
    "    plt.show()\n",
    "\n",
    "plt_data(aapl_data)\n",
    "plt_data(msft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(finance_data, company_names, window=60, forecast=1, step=1, test_size=0.25, random_state=666):\n",
    "    '''kakacode\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        finance_data : pandas.core.frame.DataFrame\n",
    "                       Данные в формате сайта finance.yahoo.com\n",
    "        window       : int                             (default 30)\n",
    "                       Размер окна \n",
    "        forecast     : int                              (default 1)\n",
    "                       Промежуток через который будем пердсказывать\n",
    "        step         : int                              (default 1)\n",
    "                       Ifu\n",
    "                       \n",
    "                       \n",
    "    '''\n",
    "    X, ys = [], []\n",
    "    data_list = [df.loc[:, 'Adj Close'].tolist() for df in finance_data]\n",
    "    for i in range(0, len(data_list[0]) - forecast - window, step):\n",
    "        x, y = [], []\n",
    "            \n",
    "        for data in data_list:\n",
    "            x_i = data[i:i+window]\n",
    "            y_i = data[i+window+forecast]  \n",
    "\n",
    "            last_close = x_i[window-1]\n",
    "            next_close = y_i\n",
    "            if last_close < next_close:\n",
    "                y_i = [1]\n",
    "            else:\n",
    "                y_i = [0] \n",
    "            x_i = (np.array(x_i) - np.mean(x_i)) / np.std(x_i)\n",
    "            x.append(x_i)\n",
    "            y.append(y_i)\n",
    "        \n",
    "        x = [[x[c_i][w_i] for c_i in range(len(data_list))] for w_i in range(window)]\n",
    "        X.append(x)\n",
    "        ys.append(y)\n",
    "    \n",
    "    X, ys = np.array(X), np.array(ys)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, ys, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    y_train = {company_names[c_i]:np.array([[y_train[i][c_i][w_i] for w_i in range(1)] for i in range(len(y_train))]) for c_i in range(len(company_names))} \n",
    "    y_test = {company_names[c_i]:np.array([[y_test[i][c_i][w_i] for w_i in range(1)] for i in range(len(y_test))]) for c_i in range(len(company_names))} \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#company_names = [\"aapl\"]\n",
    "#company_datas = [aapl_data]\n",
    "\n",
    "company_names =  [\"aapl\", \"msft\"]  \n",
    "company_datas =  [aapl_data, msft_data] \n",
    "X_train, X_test, y_train, y_test = create_windows(company_datas, company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "def create_dense_model(company_names, input_day = 60):\n",
    "    K.clear_session()\n",
    "    \n",
    "    input_model = Input(name=\"input\", shape=(input_day, len(company_names), ))\n",
    "    base = Dense(64, activity_regularizer=regularizers.l2(0.01))(input_model)\n",
    "    base = BatchNormalization()(base)\n",
    "    base = LeakyReLU()(base)\n",
    "    base = Dropout(0.5)(base)\n",
    "    base = Flatten()(base)\n",
    "    \n",
    "    outputs = []\n",
    "    loss_weights={}\n",
    "    loss={}\n",
    "    for i in range(len(company_names)):\n",
    "        x = Dense(32, activity_regularizer=regularizers.l2(0.01))(base)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('sigmoid', name=company_names[i])(x)\n",
    "        \n",
    "        outputs.append(x)\n",
    "        loss_weights[company_names[i]] = 1.0\n",
    "        loss[company_names[i]] = 'binary_crossentropy'\n",
    "    \n",
    "    model = Model(input=input_model, outputs=outputs)\n",
    "    \n",
    "    opt = 'rmsprop'\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=loss,\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_dense_model(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "def create_lstm_model(company_names, input_day = 60):\n",
    "    K.clear_session()\n",
    "    \n",
    "    input_model = Input(name=\"input\", shape=(input_day, len(company_names), ))\n",
    "    base = LSTM(30, return_sequences=True)(input_model)\n",
    "    base = LSTM(30)(base)\n",
    "    base = BatchNormalization()(base)\n",
    "    base = LeakyReLU()(base)\n",
    "    base = Dropout(0.5)(base)\n",
    "    \n",
    "    outputs = []\n",
    "    loss_weights={}\n",
    "    loss={}\n",
    "    for i in range(len(company_names)):\n",
    "        x = Dense(32, activity_regularizer=regularizers.l2(0.01))(base)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('sigmoid', name=company_names[i])(x)\n",
    "        \n",
    "        outputs.append(x)\n",
    "        loss_weights[company_names[i]] = 1.0\n",
    "        loss[company_names[i]] = 'binary_crossentropy'\n",
    "    \n",
    "    model = Model(input=input_model, outputs=outputs)\n",
    "    \n",
    "    opt = 'rmsprop'\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=loss,\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_lstm_model(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    model_m = Sequential()\n",
    "    model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\n",
    "    model_m.add(Conv1D(100, 10, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
    "    model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "    model_m.add(MaxPooling1D(3))\n",
    "    model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "    model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "    model_m.add(GlobalAveragePooling1D())\n",
    "    model_m.add(Dropout(0.5))\n",
    "    model_m.add(Dense(num_classes, activation='softmax'))\n",
    "    print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2653 samples, validate on 885 samples\n",
      "Epoch 1/100\n",
      "2653/2653 [==============================] - 2s 849us/step - loss: 30.3117 - aapl_loss: 0.7995 - msft_loss: 0.8030 - aapl_acc: 0.4949 - msft_acc: 0.5077 - val_loss: 7.4423 - val_aapl_loss: 0.7123 - val_msft_loss: 0.6979 - val_aapl_acc: 0.4780 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.44234, saving model to test.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_aapl_loss,val_msft_loss,val_aapl_acc,val_msft_acc,loss,aapl_loss,msft_loss,aapl_acc,msft_acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "2653/2653 [==============================] - 1s 299us/step - loss: 9.2437 - aapl_loss: 0.7687 - msft_loss: 0.7749 - aapl_acc: 0.5002 - msft_acc: 0.5205 - val_loss: 10.1900 - val_aapl_loss: 0.7975 - val_msft_loss: 0.7627 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 7.44234\n",
      "Epoch 3/100\n",
      "2653/2653 [==============================] - 1s 307us/step - loss: 2.8474 - aapl_loss: 0.7426 - msft_loss: 0.7373 - aapl_acc: 0.5198 - msft_acc: 0.5006 - val_loss: 8.8753 - val_aapl_loss: 0.7605 - val_msft_loss: 0.8243 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 7.44234\n",
      "Epoch 4/100\n",
      "2653/2653 [==============================] - 1s 309us/step - loss: 1.9360 - aapl_loss: 0.7439 - msft_loss: 0.7307 - aapl_acc: 0.5081 - msft_acc: 0.5062 - val_loss: 6.3259 - val_aapl_loss: 0.7362 - val_msft_loss: 0.7924 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00004: val_loss improved from 7.44234 to 6.32591, saving model to test.hdf5\n",
      "Epoch 5/100\n",
      "2653/2653 [==============================] - 1s 326us/step - loss: 1.7215 - aapl_loss: 0.7193 - msft_loss: 0.7036 - aapl_acc: 0.5254 - msft_acc: 0.5224 - val_loss: 5.7918 - val_aapl_loss: 0.7162 - val_msft_loss: 0.7638 - val_aapl_acc: 0.4554 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.32591 to 5.79185, saving model to test.hdf5\n",
      "Epoch 6/100\n",
      "2653/2653 [==============================] - 1s 353us/step - loss: 1.6019 - aapl_loss: 0.7057 - msft_loss: 0.7099 - aapl_acc: 0.5266 - msft_acc: 0.5168 - val_loss: 4.0138 - val_aapl_loss: 0.7076 - val_msft_loss: 0.7763 - val_aapl_acc: 0.4508 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.79185 to 4.01381, saving model to test.hdf5\n",
      "Epoch 7/100\n",
      "2653/2653 [==============================] - 1s 319us/step - loss: 1.5348 - aapl_loss: 0.7010 - msft_loss: 0.7059 - aapl_acc: 0.5232 - msft_acc: 0.4964 - val_loss: 3.6295 - val_aapl_loss: 0.6988 - val_msft_loss: 0.7509 - val_aapl_acc: 0.4712 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.01381 to 3.62955, saving model to test.hdf5\n",
      "Epoch 8/100\n",
      "2653/2653 [==============================] - 1s 287us/step - loss: 1.4992 - aapl_loss: 0.6995 - msft_loss: 0.6941 - aapl_acc: 0.5307 - msft_acc: 0.5247 - val_loss: 4.3044 - val_aapl_loss: 0.6913 - val_msft_loss: 0.7396 - val_aapl_acc: 0.5243 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.62955\n",
      "Epoch 9/100\n",
      "2653/2653 [==============================] - 1s 284us/step - loss: 1.4820 - aapl_loss: 0.6964 - msft_loss: 0.6986 - aapl_acc: 0.5341 - msft_acc: 0.5043 - val_loss: 4.0376 - val_aapl_loss: 0.6906 - val_msft_loss: 0.7274 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.62955\n",
      "Epoch 10/100\n",
      "2653/2653 [==============================] - 1s 302us/step - loss: 1.4534 - aapl_loss: 0.6903 - msft_loss: 0.6953 - aapl_acc: 0.5420 - msft_acc: 0.5149 - val_loss: 3.6975 - val_aapl_loss: 0.6942 - val_msft_loss: 0.7331 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.62955\n",
      "Epoch 11/100\n",
      "2653/2653 [==============================] - 1s 274us/step - loss: 1.4518 - aapl_loss: 0.6956 - msft_loss: 0.6918 - aapl_acc: 0.5341 - msft_acc: 0.5319 - val_loss: 4.0133 - val_aapl_loss: 0.7030 - val_msft_loss: 0.7265 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.62955\n",
      "Epoch 12/100\n",
      "2653/2653 [==============================] - 1s 271us/step - loss: 1.4421 - aapl_loss: 0.6923 - msft_loss: 0.6905 - aapl_acc: 0.5356 - msft_acc: 0.5300 - val_loss: 4.6975 - val_aapl_loss: 0.7051 - val_msft_loss: 0.7681 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.62955\n",
      "Epoch 13/100\n",
      "2653/2653 [==============================] - 1s 276us/step - loss: 1.4293 - aapl_loss: 0.6907 - msft_loss: 0.6940 - aapl_acc: 0.5326 - msft_acc: 0.5266 - val_loss: 3.7530 - val_aapl_loss: 0.7089 - val_msft_loss: 0.7465 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.62955\n",
      "Epoch 14/100\n",
      "2653/2653 [==============================] - 1s 272us/step - loss: 1.4308 - aapl_loss: 0.6907 - msft_loss: 0.6950 - aapl_acc: 0.5462 - msft_acc: 0.5288 - val_loss: 3.7447 - val_aapl_loss: 0.7032 - val_msft_loss: 0.7281 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.62955\n",
      "Epoch 15/100\n",
      "2653/2653 [==============================] - 1s 307us/step - loss: 1.4258 - aapl_loss: 0.6932 - msft_loss: 0.6945 - aapl_acc: 0.5413 - msft_acc: 0.5130 - val_loss: 4.1820 - val_aapl_loss: 0.6903 - val_msft_loss: 0.7156 - val_aapl_acc: 0.5446 - val_msft_acc: 0.5073\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.62955\n",
      "Epoch 16/100\n",
      "2653/2653 [==============================] - 1s 309us/step - loss: 1.4178 - aapl_loss: 0.6904 - msft_loss: 0.6921 - aapl_acc: 0.5432 - msft_acc: 0.5217 - val_loss: 3.9037 - val_aapl_loss: 0.6895 - val_msft_loss: 0.6947 - val_aapl_acc: 0.5356 - val_msft_acc: 0.4893\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.62955\n",
      "Epoch 17/100\n",
      "2653/2653 [==============================] - 1s 287us/step - loss: 1.4111 - aapl_loss: 0.6872 - msft_loss: 0.6911 - aapl_acc: 0.5503 - msft_acc: 0.5262 - val_loss: 3.5657 - val_aapl_loss: 0.7028 - val_msft_loss: 0.7069 - val_aapl_acc: 0.4576 - val_msft_acc: 0.4904\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.62955 to 3.56567, saving model to test.hdf5\n",
      "Epoch 18/100\n",
      "2653/2653 [==============================] - 1s 273us/step - loss: 1.4116 - aapl_loss: 0.6886 - msft_loss: 0.6936 - aapl_acc: 0.5499 - msft_acc: 0.5141 - val_loss: 3.5549 - val_aapl_loss: 0.6905 - val_msft_loss: 0.7025 - val_aapl_acc: 0.5469 - val_msft_acc: 0.5062\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.56567 to 3.55485, saving model to test.hdf5\n",
      "Epoch 19/100\n",
      "2653/2653 [==============================] - 1s 271us/step - loss: 1.4097 - aapl_loss: 0.6907 - msft_loss: 0.6922 - aapl_acc: 0.5439 - msft_acc: 0.5224 - val_loss: 3.4622 - val_aapl_loss: 0.6899 - val_msft_loss: 0.7008 - val_aapl_acc: 0.5435 - val_msft_acc: 0.5085\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.55485 to 3.46225, saving model to test.hdf5\n",
      "Epoch 20/100\n",
      "2653/2653 [==============================] - 1s 273us/step - loss: 1.4048 - aapl_loss: 0.6883 - msft_loss: 0.6904 - aapl_acc: 0.5488 - msft_acc: 0.5341 - val_loss: 3.7193 - val_aapl_loss: 0.7619 - val_msft_loss: 0.7447 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.46225\n",
      "Epoch 21/100\n",
      "2653/2653 [==============================] - 1s 280us/step - loss: 1.4093 - aapl_loss: 0.6920 - msft_loss: 0.6953 - aapl_acc: 0.5390 - msft_acc: 0.5021 - val_loss: 3.3754 - val_aapl_loss: 0.7430 - val_msft_loss: 0.6970 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5028\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.46225 to 3.37536, saving model to test.hdf5\n",
      "Epoch 22/100\n",
      "2653/2653 [==============================] - 1s 308us/step - loss: 1.4021 - aapl_loss: 0.6898 - msft_loss: 0.6926 - aapl_acc: 0.5496 - msft_acc: 0.5277 - val_loss: 3.4162 - val_aapl_loss: 0.7200 - val_msft_loss: 0.7478 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 3.37536\n",
      "Epoch 23/100\n",
      "2653/2653 [==============================] - 1s 308us/step - loss: 1.4027 - aapl_loss: 0.6889 - msft_loss: 0.6929 - aapl_acc: 0.5462 - msft_acc: 0.5160 - val_loss: 3.5688 - val_aapl_loss: 0.6964 - val_msft_loss: 0.8186 - val_aapl_acc: 0.5186 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 3.37536\n",
      "Epoch 24/100\n",
      "2653/2653 [==============================] - 1s 279us/step - loss: 1.3996 - aapl_loss: 0.6881 - msft_loss: 0.6928 - aapl_acc: 0.5466 - msft_acc: 0.5217 - val_loss: 3.4412 - val_aapl_loss: 0.7103 - val_msft_loss: 0.7403 - val_aapl_acc: 0.5424 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.37536\n",
      "Epoch 25/100\n",
      "2653/2653 [==============================] - 1s 278us/step - loss: 1.3979 - aapl_loss: 0.6890 - msft_loss: 0.6920 - aapl_acc: 0.5466 - msft_acc: 0.5239 - val_loss: 3.4060 - val_aapl_loss: 0.7055 - val_msft_loss: 0.7320 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3.37536\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653/2653 [==============================] - 1s 274us/step - loss: 1.3972 - aapl_loss: 0.6898 - msft_loss: 0.6928 - aapl_acc: 0.5443 - msft_acc: 0.5168 - val_loss: 3.0376 - val_aapl_loss: 0.7102 - val_msft_loss: 0.7433 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00026: val_loss improved from 3.37536 to 3.03764, saving model to test.hdf5\n",
      "Epoch 27/100\n",
      "2653/2653 [==============================] - 1s 273us/step - loss: 1.3963 - aapl_loss: 0.6890 - msft_loss: 0.6935 - aapl_acc: 0.5488 - msft_acc: 0.5100 - val_loss: 3.4114 - val_aapl_loss: 0.7511 - val_msft_loss: 0.7245 - val_aapl_acc: 0.4599 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 3.03764\n",
      "Epoch 28/100\n",
      "2653/2653 [==============================] - 1s 309us/step - loss: 1.3944 - aapl_loss: 0.6892 - msft_loss: 0.6924 - aapl_acc: 0.5443 - msft_acc: 0.5198 - val_loss: 2.7471 - val_aapl_loss: 0.6932 - val_msft_loss: 0.7301 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.03764 to 2.74711, saving model to test.hdf5\n",
      "Epoch 29/100\n",
      "2653/2653 [==============================] - 1s 304us/step - loss: 1.3961 - aapl_loss: 0.6894 - msft_loss: 0.6936 - aapl_acc: 0.5405 - msft_acc: 0.5251 - val_loss: 3.0112 - val_aapl_loss: 0.7028 - val_msft_loss: 0.7581 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.74711\n",
      "Epoch 30/100\n",
      "2653/2653 [==============================] - 1s 453us/step - loss: 1.3926 - aapl_loss: 0.6896 - msft_loss: 0.6916 - aapl_acc: 0.5428 - msft_acc: 0.5277 - val_loss: 3.0542 - val_aapl_loss: 0.7144 - val_msft_loss: 0.8659 - val_aapl_acc: 0.5435 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.74711\n",
      "Epoch 31/100\n",
      "2653/2653 [==============================] - 1s 294us/step - loss: 1.3901 - aapl_loss: 0.6881 - msft_loss: 0.6905 - aapl_acc: 0.5477 - msft_acc: 0.5277 - val_loss: 2.8524 - val_aapl_loss: 0.6964 - val_msft_loss: 0.8349 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.74711\n",
      "Epoch 32/100\n",
      "2653/2653 [==============================] - 1s 334us/step - loss: 1.3912 - aapl_loss: 0.6876 - msft_loss: 0.6916 - aapl_acc: 0.5515 - msft_acc: 0.5258 - val_loss: 3.1291 - val_aapl_loss: 0.6944 - val_msft_loss: 0.9817 - val_aapl_acc: 0.4927 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.74711\n",
      "Epoch 33/100\n",
      "2653/2653 [==============================] - 1s 303us/step - loss: 1.3922 - aapl_loss: 0.6890 - msft_loss: 0.6931 - aapl_acc: 0.5488 - msft_acc: 0.5164 - val_loss: 3.2178 - val_aapl_loss: 0.7032 - val_msft_loss: 0.7681 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.74711\n",
      "Epoch 34/100\n",
      "2653/2653 [==============================] - 1s 298us/step - loss: 1.3898 - aapl_loss: 0.6882 - msft_loss: 0.6927 - aapl_acc: 0.5496 - msft_acc: 0.5202 - val_loss: 2.9288 - val_aapl_loss: 0.7532 - val_msft_loss: 0.7621 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.74711\n",
      "Epoch 35/100\n",
      "2653/2653 [==============================] - 1s 331us/step - loss: 1.3897 - aapl_loss: 0.6896 - msft_loss: 0.6905 - aapl_acc: 0.5386 - msft_acc: 0.5303 - val_loss: 3.2494 - val_aapl_loss: 0.6971 - val_msft_loss: 0.9374 - val_aapl_acc: 0.5232 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.74711\n",
      "Epoch 36/100\n",
      "2653/2653 [==============================] - 1s 305us/step - loss: 1.3887 - aapl_loss: 0.6901 - msft_loss: 0.6916 - aapl_acc: 0.5443 - msft_acc: 0.5285 - val_loss: 3.0836 - val_aapl_loss: 0.6932 - val_msft_loss: 0.9330 - val_aapl_acc: 0.5107 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.74711\n",
      "Epoch 37/100\n",
      "2653/2653 [==============================] - 1s 302us/step - loss: 1.3891 - aapl_loss: 0.6883 - msft_loss: 0.6934 - aapl_acc: 0.5477 - msft_acc: 0.5164 - val_loss: 3.2882 - val_aapl_loss: 0.6936 - val_msft_loss: 1.0091 - val_aapl_acc: 0.5333 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.74711\n",
      "Epoch 38/100\n",
      "2653/2653 [==============================] - 1s 335us/step - loss: 1.3882 - aapl_loss: 0.6887 - msft_loss: 0.6924 - aapl_acc: 0.5481 - msft_acc: 0.5194 - val_loss: 3.2120 - val_aapl_loss: 0.6965 - val_msft_loss: 1.0090 - val_aapl_acc: 0.4802 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.74711\n",
      "Epoch 39/100\n",
      "2653/2653 [==============================] - 1s 294us/step - loss: 1.3872 - aapl_loss: 0.6883 - msft_loss: 0.6915 - aapl_acc: 0.5439 - msft_acc: 0.5254 - val_loss: 2.7892 - val_aapl_loss: 0.6962 - val_msft_loss: 0.8604 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.74711\n",
      "Epoch 40/100\n",
      "2653/2653 [==============================] - 1s 309us/step - loss: 1.3876 - aapl_loss: 0.6880 - msft_loss: 0.6924 - aapl_acc: 0.5552 - msft_acc: 0.5194 - val_loss: 2.9004 - val_aapl_loss: 0.7429 - val_msft_loss: 0.8164 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.74711\n",
      "Epoch 41/100\n",
      "2653/2653 [==============================] - 1s 274us/step - loss: 1.3833 - aapl_loss: 0.6869 - msft_loss: 0.6898 - aapl_acc: 0.5533 - msft_acc: 0.5432 - val_loss: 2.5233 - val_aapl_loss: 0.7108 - val_msft_loss: 0.7001 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4972\n",
      "\n",
      "Epoch 00041: val_loss improved from 2.74711 to 2.52332, saving model to test.hdf5\n",
      "Epoch 42/100\n",
      "2653/2653 [==============================] - 1s 301us/step - loss: 1.3868 - aapl_loss: 0.6891 - msft_loss: 0.6924 - aapl_acc: 0.5481 - msft_acc: 0.5202 - val_loss: 2.6224 - val_aapl_loss: 0.7247 - val_msft_loss: 0.7044 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.52332\n",
      "Epoch 43/100\n",
      "2653/2653 [==============================] - 1s 289us/step - loss: 1.3847 - aapl_loss: 0.6880 - msft_loss: 0.6918 - aapl_acc: 0.5469 - msft_acc: 0.5292 - val_loss: 2.5454 - val_aapl_loss: 0.7250 - val_msft_loss: 0.7103 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.52332\n",
      "Epoch 44/100\n",
      "2653/2653 [==============================] - 1s 278us/step - loss: 1.3853 - aapl_loss: 0.6885 - msft_loss: 0.6916 - aapl_acc: 0.5432 - msft_acc: 0.5224 - val_loss: 2.4695 - val_aapl_loss: 0.7068 - val_msft_loss: 0.7035 - val_aapl_acc: 0.4588 - val_msft_acc: 0.4972\n",
      "\n",
      "Epoch 00044: val_loss improved from 2.52332 to 2.46948, saving model to test.hdf5\n",
      "Epoch 45/100\n",
      "2653/2653 [==============================] - 1s 270us/step - loss: 1.3842 - aapl_loss: 0.6868 - msft_loss: 0.6921 - aapl_acc: 0.5492 - msft_acc: 0.5179 - val_loss: 2.5355 - val_aapl_loss: 0.7692 - val_msft_loss: 0.7075 - val_aapl_acc: 0.4599 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.46948\n",
      "Epoch 46/100\n",
      "2653/2653 [==============================] - 1s 270us/step - loss: 1.3835 - aapl_loss: 0.6873 - msft_loss: 0.6916 - aapl_acc: 0.5496 - msft_acc: 0.5262 - val_loss: 2.5345 - val_aapl_loss: 0.7183 - val_msft_loss: 0.7177 - val_aapl_acc: 0.4610 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.46948\n",
      "Epoch 47/100\n",
      "2653/2653 [==============================] - 1s 268us/step - loss: 1.3816 - aapl_loss: 0.6874 - msft_loss: 0.6900 - aapl_acc: 0.5454 - msft_acc: 0.5205 - val_loss: 2.2723 - val_aapl_loss: 0.6958 - val_msft_loss: 0.6997 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4949\n",
      "\n",
      "Epoch 00047: val_loss improved from 2.46948 to 2.27229, saving model to test.hdf5\n",
      "Epoch 48/100\n",
      "2653/2653 [==============================] - 1s 275us/step - loss: 1.3859 - aapl_loss: 0.6891 - msft_loss: 0.6922 - aapl_acc: 0.5428 - msft_acc: 0.5232 - val_loss: 2.3763 - val_aapl_loss: 0.6943 - val_msft_loss: 0.7099 - val_aapl_acc: 0.5412 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.27229\n",
      "Epoch 49/100\n",
      "2653/2653 [==============================] - 1s 284us/step - loss: 1.3836 - aapl_loss: 0.6894 - msft_loss: 0.6907 - aapl_acc: 0.5428 - msft_acc: 0.5292 - val_loss: 2.0751 - val_aapl_loss: 0.6925 - val_msft_loss: 0.7055 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00049: val_loss improved from 2.27229 to 2.07508, saving model to test.hdf5\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653/2653 [==============================] - 1s 273us/step - loss: 1.3827 - aapl_loss: 0.6879 - msft_loss: 0.6913 - aapl_acc: 0.5424 - msft_acc: 0.5330 - val_loss: 2.1626 - val_aapl_loss: 0.6958 - val_msft_loss: 0.7248 - val_aapl_acc: 0.4859 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.07508\n",
      "Epoch 51/100\n",
      "2653/2653 [==============================] - 1s 271us/step - loss: 1.3833 - aapl_loss: 0.6889 - msft_loss: 0.6908 - aapl_acc: 0.5401 - msft_acc: 0.5364 - val_loss: 1.9884 - val_aapl_loss: 0.7126 - val_msft_loss: 0.7176 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00051: val_loss improved from 2.07508 to 1.98841, saving model to test.hdf5\n",
      "Epoch 52/100\n",
      "2653/2653 [==============================] - 1s 272us/step - loss: 1.3826 - aapl_loss: 0.6887 - msft_loss: 0.6905 - aapl_acc: 0.5432 - msft_acc: 0.5288 - val_loss: 1.7951 - val_aapl_loss: 0.6906 - val_msft_loss: 0.6955 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5175\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.98841 to 1.79510, saving model to test.hdf5\n",
      "Epoch 53/100\n",
      "2653/2653 [==============================] - 1s 280us/step - loss: 1.3820 - aapl_loss: 0.6877 - msft_loss: 0.6902 - aapl_acc: 0.5492 - msft_acc: 0.5273 - val_loss: 1.8868 - val_aapl_loss: 0.6938 - val_msft_loss: 0.6944 - val_aapl_acc: 0.4949 - val_msft_acc: 0.5062\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.79510\n",
      "Epoch 54/100\n",
      "2653/2653 [==============================] - 1s 282us/step - loss: 1.3840 - aapl_loss: 0.6887 - msft_loss: 0.6917 - aapl_acc: 0.5462 - msft_acc: 0.5270 - val_loss: 1.8875 - val_aapl_loss: 0.6911 - val_msft_loss: 0.6926 - val_aapl_acc: 0.5209 - val_msft_acc: 0.5130\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.79510\n",
      "Epoch 55/100\n",
      "2653/2653 [==============================] - 1s 276us/step - loss: 1.3811 - aapl_loss: 0.6871 - msft_loss: 0.6909 - aapl_acc: 0.5439 - msft_acc: 0.5251 - val_loss: 1.7785 - val_aapl_loss: 0.6923 - val_msft_loss: 0.7096 - val_aapl_acc: 0.5277 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.79510 to 1.77852, saving model to test.hdf5\n",
      "Epoch 56/100\n",
      "2653/2653 [==============================] - 1s 306us/step - loss: 1.3799 - aapl_loss: 0.6870 - msft_loss: 0.6896 - aapl_acc: 0.5515 - msft_acc: 0.5349 - val_loss: 1.6025 - val_aapl_loss: 0.6959 - val_msft_loss: 0.6988 - val_aapl_acc: 0.5401 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.77852 to 1.60250, saving model to test.hdf5\n",
      "Epoch 57/100\n",
      "2653/2653 [==============================] - 1s 279us/step - loss: 1.3825 - aapl_loss: 0.6885 - msft_loss: 0.6901 - aapl_acc: 0.5432 - msft_acc: 0.5277 - val_loss: 1.7083 - val_aapl_loss: 0.6940 - val_msft_loss: 0.7312 - val_aapl_acc: 0.5424 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.60250\n",
      "Epoch 58/100\n",
      "2653/2653 [==============================] - 1s 279us/step - loss: 1.3819 - aapl_loss: 0.6883 - msft_loss: 0.6910 - aapl_acc: 0.5469 - msft_acc: 0.5262 - val_loss: 1.7362 - val_aapl_loss: 0.7358 - val_msft_loss: 0.7052 - val_aapl_acc: 0.4599 - val_msft_acc: 0.4904\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.60250\n",
      "Epoch 59/100\n",
      "2653/2653 [==============================] - 1s 388us/step - loss: 1.3785 - aapl_loss: 0.6869 - msft_loss: 0.6887 - aapl_acc: 0.5526 - msft_acc: 0.5386 - val_loss: 1.6013 - val_aapl_loss: 0.6965 - val_msft_loss: 0.6971 - val_aapl_acc: 0.4814 - val_msft_acc: 0.5028\n",
      "\n",
      "Epoch 00059: val_loss improved from 1.60250 to 1.60130, saving model to test.hdf5\n",
      "Epoch 60/100\n",
      "2653/2653 [==============================] - 1s 294us/step - loss: 1.3817 - aapl_loss: 0.6880 - msft_loss: 0.6907 - aapl_acc: 0.5424 - msft_acc: 0.5270 - val_loss: 1.6586 - val_aapl_loss: 0.6897 - val_msft_loss: 0.6935 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5243\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.60130\n",
      "Epoch 61/100\n",
      "2653/2653 [==============================] - 1s 283us/step - loss: 1.3813 - aapl_loss: 0.6882 - msft_loss: 0.6902 - aapl_acc: 0.5481 - msft_acc: 0.5266 - val_loss: 1.6040 - val_aapl_loss: 0.7074 - val_msft_loss: 0.6940 - val_aapl_acc: 0.4825 - val_msft_acc: 0.5186\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.60130\n",
      "Epoch 62/100\n",
      "2653/2653 [==============================] - 1s 292us/step - loss: 1.3802 - aapl_loss: 0.6865 - msft_loss: 0.6901 - aapl_acc: 0.5496 - msft_acc: 0.5322 - val_loss: 1.6019 - val_aapl_loss: 0.7232 - val_msft_loss: 0.6940 - val_aapl_acc: 0.4610 - val_msft_acc: 0.5096\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.60130\n",
      "Epoch 63/100\n",
      "2653/2653 [==============================] - 1s 310us/step - loss: 1.3795 - aapl_loss: 0.6867 - msft_loss: 0.6902 - aapl_acc: 0.5548 - msft_acc: 0.5349 - val_loss: 1.5813 - val_aapl_loss: 0.7005 - val_msft_loss: 0.6986 - val_aapl_acc: 0.4621 - val_msft_acc: 0.5040\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.60130 to 1.58127, saving model to test.hdf5\n",
      "Epoch 64/100\n",
      "2653/2653 [==============================] - 1s 294us/step - loss: 1.3783 - aapl_loss: 0.6867 - msft_loss: 0.6884 - aapl_acc: 0.5518 - msft_acc: 0.5368 - val_loss: 1.5900 - val_aapl_loss: 0.7024 - val_msft_loss: 0.6967 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5073\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.58127\n",
      "Epoch 65/100\n",
      "2653/2653 [==============================] - 1s 290us/step - loss: 1.3795 - aapl_loss: 0.6865 - msft_loss: 0.6904 - aapl_acc: 0.5526 - msft_acc: 0.5326 - val_loss: 1.5702 - val_aapl_loss: 0.7066 - val_msft_loss: 0.6939 - val_aapl_acc: 0.4633 - val_msft_acc: 0.5096\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.58127 to 1.57021, saving model to test.hdf5\n",
      "Epoch 66/100\n",
      "2653/2653 [==============================] - 1s 289us/step - loss: 1.3795 - aapl_loss: 0.6873 - msft_loss: 0.6901 - aapl_acc: 0.5469 - msft_acc: 0.5254 - val_loss: 1.5628 - val_aapl_loss: 0.6921 - val_msft_loss: 0.6933 - val_aapl_acc: 0.5220 - val_msft_acc: 0.5243\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.57021 to 1.56281, saving model to test.hdf5\n",
      "Epoch 67/100\n",
      "2653/2653 [==============================] - 1s 287us/step - loss: 1.3804 - aapl_loss: 0.6875 - msft_loss: 0.6905 - aapl_acc: 0.5492 - msft_acc: 0.5307 - val_loss: 1.5222 - val_aapl_loss: 0.6951 - val_msft_loss: 0.7029 - val_aapl_acc: 0.4994 - val_msft_acc: 0.5040\n",
      "\n",
      "Epoch 00067: val_loss improved from 1.56281 to 1.52221, saving model to test.hdf5\n",
      "Epoch 68/100\n",
      "2653/2653 [==============================] - 1s 270us/step - loss: 1.3799 - aapl_loss: 0.6868 - msft_loss: 0.6906 - aapl_acc: 0.5560 - msft_acc: 0.5405 - val_loss: 1.5733 - val_aapl_loss: 0.7315 - val_msft_loss: 0.6963 - val_aapl_acc: 0.4599 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.52221\n",
      "Epoch 69/100\n",
      "2653/2653 [==============================] - 1s 272us/step - loss: 1.3786 - aapl_loss: 0.6867 - msft_loss: 0.6895 - aapl_acc: 0.5496 - msft_acc: 0.5401 - val_loss: 1.5351 - val_aapl_loss: 0.7290 - val_msft_loss: 0.6977 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5040\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.52221\n",
      "Epoch 70/100\n",
      "2653/2653 [==============================] - 1s 270us/step - loss: 1.3794 - aapl_loss: 0.6866 - msft_loss: 0.6903 - aapl_acc: 0.5560 - msft_acc: 0.5202 - val_loss: 1.5456 - val_aapl_loss: 0.7427 - val_msft_loss: 0.6938 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5130\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.52221\n",
      "Epoch 71/100\n",
      "2653/2653 [==============================] - 1s 275us/step - loss: 1.3800 - aapl_loss: 0.6870 - msft_loss: 0.6907 - aapl_acc: 0.5507 - msft_acc: 0.5368 - val_loss: 1.5443 - val_aapl_loss: 0.7670 - val_msft_loss: 0.6939 - val_aapl_acc: 0.4599 - val_msft_acc: 0.5119\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.52221\n",
      "Epoch 72/100\n",
      "2653/2653 [==============================] - 1s 270us/step - loss: 1.3785 - aapl_loss: 0.6853 - msft_loss: 0.6905 - aapl_acc: 0.5533 - msft_acc: 0.5360 - val_loss: 1.5514 - val_aapl_loss: 0.7397 - val_msft_loss: 0.7063 - val_aapl_acc: 0.4610 - val_msft_acc: 0.4972\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.52221\n",
      "Epoch 73/100\n",
      "2653/2653 [==============================] - 1s 269us/step - loss: 1.3790 - aapl_loss: 0.6861 - msft_loss: 0.6905 - aapl_acc: 0.5507 - msft_acc: 0.5390 - val_loss: 1.5210 - val_aapl_loss: 0.7307 - val_msft_loss: 0.7009 - val_aapl_acc: 0.4621 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00073: val_loss improved from 1.52221 to 1.52103, saving model to test.hdf5\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653/2653 [==============================] - 1s 270us/step - loss: 1.3786 - aapl_loss: 0.6865 - msft_loss: 0.6900 - aapl_acc: 0.5473 - msft_acc: 0.5296 - val_loss: 1.5227 - val_aapl_loss: 0.7411 - val_msft_loss: 0.6936 - val_aapl_acc: 0.4610 - val_msft_acc: 0.5141\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.52103\n",
      "Epoch 75/100\n",
      "2653/2653 [==============================] - 1s 271us/step - loss: 1.3787 - aapl_loss: 0.6876 - msft_loss: 0.6887 - aapl_acc: 0.5613 - msft_acc: 0.5386 - val_loss: 1.4570 - val_aapl_loss: 0.7052 - val_msft_loss: 0.6952 - val_aapl_acc: 0.4621 - val_msft_acc: 0.5198\n",
      "\n",
      "Epoch 00075: val_loss improved from 1.52103 to 1.45703, saving model to test.hdf5\n",
      "Epoch 76/100\n",
      "2653/2653 [==============================] - 1s 349us/step - loss: 1.3779 - aapl_loss: 0.6866 - msft_loss: 0.6891 - aapl_acc: 0.5503 - msft_acc: 0.5330 - val_loss: 1.4718 - val_aapl_loss: 0.7120 - val_msft_loss: 0.7107 - val_aapl_acc: 0.4621 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.45703\n",
      "Epoch 77/100\n",
      "2653/2653 [==============================] - 1s 279us/step - loss: 1.3789 - aapl_loss: 0.6867 - msft_loss: 0.6901 - aapl_acc: 0.5545 - msft_acc: 0.5375 - val_loss: 1.4792 - val_aapl_loss: 0.7129 - val_msft_loss: 0.7141 - val_aapl_acc: 0.4723 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.45703\n",
      "Epoch 78/100\n",
      "2653/2653 [==============================] - 1s 305us/step - loss: 1.3780 - aapl_loss: 0.6869 - msft_loss: 0.6890 - aapl_acc: 0.5481 - msft_acc: 0.5458 - val_loss: 1.4588 - val_aapl_loss: 0.7051 - val_msft_loss: 0.7196 - val_aapl_acc: 0.4847 - val_msft_acc: 0.5051\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.45703\n",
      "Epoch 79/100\n",
      "2653/2653 [==============================] - 1s 291us/step - loss: 1.3796 - aapl_loss: 0.6873 - msft_loss: 0.6898 - aapl_acc: 0.5488 - msft_acc: 0.5273 - val_loss: 1.4961 - val_aapl_loss: 0.6952 - val_msft_loss: 0.7384 - val_aapl_acc: 0.5051 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.45703\n",
      "Epoch 80/100\n",
      "2653/2653 [==============================] - 1s 292us/step - loss: 1.3777 - aapl_loss: 0.6870 - msft_loss: 0.6888 - aapl_acc: 0.5575 - msft_acc: 0.5311 - val_loss: 1.5125 - val_aapl_loss: 0.6890 - val_msft_loss: 0.7536 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.45703\n",
      "Epoch 81/100\n",
      "2653/2653 [==============================] - 1s 383us/step - loss: 1.3791 - aapl_loss: 0.6878 - msft_loss: 0.6894 - aapl_acc: 0.5537 - msft_acc: 0.5341 - val_loss: 1.4970 - val_aapl_loss: 0.6933 - val_msft_loss: 0.7410 - val_aapl_acc: 0.5096 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.45703\n",
      "Epoch 82/100\n",
      "2653/2653 [==============================] - 1s 512us/step - loss: 1.3781 - aapl_loss: 0.6860 - msft_loss: 0.6901 - aapl_acc: 0.5560 - msft_acc: 0.5398 - val_loss: 1.4745 - val_aapl_loss: 0.6978 - val_msft_loss: 0.7244 - val_aapl_acc: 0.5480 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.45703\n",
      "Epoch 83/100\n",
      "2653/2653 [==============================] - 1s 319us/step - loss: 1.3787 - aapl_loss: 0.6882 - msft_loss: 0.6886 - aapl_acc: 0.5484 - msft_acc: 0.5322 - val_loss: 1.4533 - val_aapl_loss: 0.6920 - val_msft_loss: 0.7073 - val_aapl_acc: 0.5480 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00083: val_loss improved from 1.45703 to 1.45327, saving model to test.hdf5\n",
      "Epoch 84/100\n",
      "2653/2653 [==============================] - 1s 310us/step - loss: 1.3784 - aapl_loss: 0.6868 - msft_loss: 0.6896 - aapl_acc: 0.5492 - msft_acc: 0.5307 - val_loss: 1.4403 - val_aapl_loss: 0.6905 - val_msft_loss: 0.7250 - val_aapl_acc: 0.5367 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00084: val_loss improved from 1.45327 to 1.44035, saving model to test.hdf5\n",
      "Epoch 85/100\n",
      "2653/2653 [==============================] - 1s 318us/step - loss: 1.3765 - aapl_loss: 0.6861 - msft_loss: 0.6885 - aapl_acc: 0.5454 - msft_acc: 0.5383 - val_loss: 1.4495 - val_aapl_loss: 0.7036 - val_msft_loss: 0.7105 - val_aapl_acc: 0.5525 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.44035\n",
      "Epoch 86/100\n",
      "2653/2653 [==============================] - 1s 348us/step - loss: 1.3788 - aapl_loss: 0.6864 - msft_loss: 0.6902 - aapl_acc: 0.5473 - msft_acc: 0.5352 - val_loss: 1.4200 - val_aapl_loss: 0.6996 - val_msft_loss: 0.6928 - val_aapl_acc: 0.4836 - val_msft_acc: 0.5220\n",
      "\n",
      "Epoch 00086: val_loss improved from 1.44035 to 1.41996, saving model to test.hdf5\n",
      "Epoch 87/100\n",
      "2653/2653 [==============================] - 2s 572us/step - loss: 1.3761 - aapl_loss: 0.6860 - msft_loss: 0.6881 - aapl_acc: 0.5597 - msft_acc: 0.5413 - val_loss: 1.4172 - val_aapl_loss: 0.6971 - val_msft_loss: 0.6952 - val_aapl_acc: 0.4644 - val_msft_acc: 0.5051\n",
      "\n",
      "Epoch 00087: val_loss improved from 1.41996 to 1.41723, saving model to test.hdf5\n",
      "Epoch 88/100\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 1.3778 - aapl_loss: 0.6865 - msft_loss: 0.6892 - aapl_acc: 0.5502 - msft_acc: 0.532 - 1s 306us/step - loss: 1.3786 - aapl_loss: 0.6869 - msft_loss: 0.6896 - aapl_acc: 0.5488 - msft_acc: 0.5296 - val_loss: 1.4133 - val_aapl_loss: 0.6968 - val_msft_loss: 0.6989 - val_aapl_acc: 0.4712 - val_msft_acc: 0.4972\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.41723 to 1.41335, saving model to test.hdf5\n",
      "Epoch 89/100\n",
      "2653/2653 [==============================] - 1s 338us/step - loss: 1.3765 - aapl_loss: 0.6858 - msft_loss: 0.6886 - aapl_acc: 0.5507 - msft_acc: 0.5296 - val_loss: 1.4080 - val_aapl_loss: 0.6919 - val_msft_loss: 0.6974 - val_aapl_acc: 0.5232 - val_msft_acc: 0.5062\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.41335 to 1.40800, saving model to test.hdf5\n",
      "Epoch 90/100\n",
      "2653/2653 [==============================] - 1s 370us/step - loss: 1.3774 - aapl_loss: 0.6858 - msft_loss: 0.6898 - aapl_acc: 0.5556 - msft_acc: 0.5379 - val_loss: 1.4171 - val_aapl_loss: 0.6939 - val_msft_loss: 0.6938 - val_aapl_acc: 0.5073 - val_msft_acc: 0.5299\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.40800\n",
      "Epoch 91/100\n",
      "2653/2653 [==============================] - 1s 343us/step - loss: 1.3754 - aapl_loss: 0.6852 - msft_loss: 0.6883 - aapl_acc: 0.5560 - msft_acc: 0.5386 - val_loss: 1.4140 - val_aapl_loss: 0.6902 - val_msft_loss: 0.6992 - val_aapl_acc: 0.5492 - val_msft_acc: 0.4960\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.40800\n",
      "Epoch 92/100\n",
      "2653/2653 [==============================] - 1s 277us/step - loss: 1.3753 - aapl_loss: 0.6866 - msft_loss: 0.6865 - aapl_acc: 0.5496 - msft_acc: 0.5394 - val_loss: 1.4269 - val_aapl_loss: 0.6930 - val_msft_loss: 0.7070 - val_aapl_acc: 0.5130 - val_msft_acc: 0.4983\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.40800\n",
      "Epoch 93/100\n",
      "2653/2653 [==============================] - 1s 292us/step - loss: 1.3766 - aapl_loss: 0.6861 - msft_loss: 0.6886 - aapl_acc: 0.5447 - msft_acc: 0.5447 - val_loss: 1.4178 - val_aapl_loss: 0.6898 - val_msft_loss: 0.7075 - val_aapl_acc: 0.5446 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.40800\n",
      "Epoch 94/100\n",
      "2653/2653 [==============================] - 1s 284us/step - loss: 1.3745 - aapl_loss: 0.6850 - msft_loss: 0.6874 - aapl_acc: 0.5537 - msft_acc: 0.5379 - val_loss: 1.4947 - val_aapl_loss: 0.7349 - val_msft_loss: 0.7228 - val_aapl_acc: 0.5503 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.40800\n",
      "Epoch 95/100\n",
      "2653/2653 [==============================] - 1s 296us/step - loss: 1.3773 - aapl_loss: 0.6863 - msft_loss: 0.6886 - aapl_acc: 0.5507 - msft_acc: 0.5311 - val_loss: 1.4660 - val_aapl_loss: 0.6986 - val_msft_loss: 0.7223 - val_aapl_acc: 0.5401 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.40800\n",
      "Epoch 96/100\n",
      "2653/2653 [==============================] - 1s 286us/step - loss: 1.3758 - aapl_loss: 0.6857 - msft_loss: 0.6880 - aapl_acc: 0.5575 - msft_acc: 0.5386 - val_loss: 1.4260 - val_aapl_loss: 0.6918 - val_msft_loss: 0.7062 - val_aapl_acc: 0.5379 - val_msft_acc: 0.4972\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.40800\n",
      "Epoch 97/100\n",
      "2653/2653 [==============================] - 1s 284us/step - loss: 1.3764 - aapl_loss: 0.6863 - msft_loss: 0.6881 - aapl_acc: 0.5548 - msft_acc: 0.5281 - val_loss: 1.4318 - val_aapl_loss: 0.6928 - val_msft_loss: 0.7176 - val_aapl_acc: 0.5232 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.40800\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653/2653 [==============================] - 1s 292us/step - loss: 1.3773 - aapl_loss: 0.6864 - msft_loss: 0.6889 - aapl_acc: 0.5503 - msft_acc: 0.5296 - val_loss: 1.4461 - val_aapl_loss: 0.6933 - val_msft_loss: 0.7224 - val_aapl_acc: 0.4949 - val_msft_acc: 0.4994\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.40800\n",
      "Epoch 99/100\n",
      "2653/2653 [==============================] - 1s 276us/step - loss: 1.3769 - aapl_loss: 0.6859 - msft_loss: 0.6892 - aapl_acc: 0.5522 - msft_acc: 0.5322 - val_loss: 1.4300 - val_aapl_loss: 0.6940 - val_msft_loss: 0.7072 - val_aapl_acc: 0.5311 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.40800\n",
      "Epoch 100/100\n",
      "2653/2653 [==============================] - 1s 280us/step - loss: 1.3754 - aapl_loss: 0.6849 - msft_loss: 0.6883 - aapl_acc: 0.5545 - msft_acc: 0.5341 - val_loss: 1.4382 - val_aapl_loss: 0.6984 - val_msft_loss: 0.7174 - val_aapl_acc: 0.4633 - val_msft_acc: 0.5006\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.40800\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=25, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"test.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          epochs = 100, \n",
    "          batch_size = 128, \n",
    "          verbose=1,\n",
    "          validation_split=0.25,\n",
    "          callbacks=[reduce_lr, checkpointer],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def explain_train(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "explain_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 0s 181us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.408679790011907,\n",
       " 0.6960593615548085,\n",
       " 0.7069655113301034,\n",
       " 0.49067796620271975,\n",
       " 0.5169491531485219]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать на высокочастотных данных (каждый час, каждые пять минут) — больше данных — больше паттернов — меньше переобучения\n",
    "Использовать более продвинутые архитектуры нейронных сетей, которые предназначены для работы с последовательностями — convolutional neural networks, recurrent neural networks\n",
    "Использовать не только цену закрытия, а все данные из нашего .csv (high, low, open, close, volume) — то есть в каждый момент времени обращать внимание на всю доступную информацию\n",
    "Оптимизировать гиперпараметры — размер окна, количество нейронов в скрытых слоях, шаг обучения — все эти параметры были взяты несколько наугад, с помощью случайного поиска можно выяснить, что, возможно, нам надо смотреть на 45 дней назад и учить с меньшим шагом более глубокую сетку.\n",
    "Использовать более подходящие для нашей задачи функции потерь (например, для прогнозирования изменения цены мы могли бы штрафовать нейронную за неправильный знак, обычная MSE к знаку числа инвариантна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-4c12ce375257>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-4c12ce375257>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://habr.com/ru/company/wunderfund/blog/331310/\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# https://habr.com/ru/company/wunderfund/blog/331310/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
