{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import merge\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam, Adadelta\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D, LSTM, Conv1D, GlobalAveragePooling1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные:\n",
    "* **AAPL** https://finance.yahoo.com/quote/AAPL/history?p=AAPL&.tsrc=fin-srch\n",
    "* **MSFT** https://finance.yahoo.com/quote/MSFT/history?p=MSFT&.tsrc=fin-srch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_PATH = \"/mnt/data/home/probachaydmitry/Downloads/AAPL.csv\"\n",
    "MSFT_PATH = \"/mnt/data/home/probachaydmitry/Downloads/MSFT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data = pd.read_csv(AAPL_PATH)[::-1]\n",
    "msft_data = pd.read_csv(MSFT_PATH)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>195.339996</td>\n",
       "      <td>197.690002</td>\n",
       "      <td>190.779999</td>\n",
       "      <td>191.050003</td>\n",
       "      <td>191.050003</td>\n",
       "      <td>42359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>190.020004</td>\n",
       "      <td>196.330002</td>\n",
       "      <td>189.809998</td>\n",
       "      <td>195.089996</td>\n",
       "      <td>195.089996</td>\n",
       "      <td>51034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>186.229996</td>\n",
       "      <td>189.490005</td>\n",
       "      <td>184.729996</td>\n",
       "      <td>188.160004</td>\n",
       "      <td>188.160004</td>\n",
       "      <td>31035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>188.350006</td>\n",
       "      <td>188.990005</td>\n",
       "      <td>185.919998</td>\n",
       "      <td>186.529999</td>\n",
       "      <td>186.529999</td>\n",
       "      <td>31646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>185.800003</td>\n",
       "      <td>188.389999</td>\n",
       "      <td>185.789993</td>\n",
       "      <td>188.020004</td>\n",
       "      <td>188.020004</td>\n",
       "      <td>26219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>184.850006</td>\n",
       "      <td>187.330002</td>\n",
       "      <td>183.740005</td>\n",
       "      <td>186.119995</td>\n",
       "      <td>186.119995</td>\n",
       "      <td>39042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>183.899994</td>\n",
       "      <td>184.100006</td>\n",
       "      <td>182.559998</td>\n",
       "      <td>183.729996</td>\n",
       "      <td>183.729996</td>\n",
       "      <td>23579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>182.250000</td>\n",
       "      <td>183.300003</td>\n",
       "      <td>180.919998</td>\n",
       "      <td>181.710007</td>\n",
       "      <td>181.710007</td>\n",
       "      <td>31032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>182.669998</td>\n",
       "      <td>179.369995</td>\n",
       "      <td>180.910004</td>\n",
       "      <td>180.910004</td>\n",
       "      <td>32467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>175.490005</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>175.350006</td>\n",
       "      <td>178.899994</td>\n",
       "      <td>178.899994</td>\n",
       "      <td>32011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>170.320007</td>\n",
       "      <td>173.070007</td>\n",
       "      <td>169.500000</td>\n",
       "      <td>172.910004</td>\n",
       "      <td>172.910004</td>\n",
       "      <td>23999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>173.869995</td>\n",
       "      <td>174.440002</td>\n",
       "      <td>172.020004</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>24796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>174.669998</td>\n",
       "      <td>175.490005</td>\n",
       "      <td>173.940002</td>\n",
       "      <td>174.520004</td>\n",
       "      <td>174.520004</td>\n",
       "      <td>20810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>175.940002</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>174.539993</td>\n",
       "      <td>175.529999</td>\n",
       "      <td>175.529999</td>\n",
       "      <td>19737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>175.690002</td>\n",
       "      <td>177.750000</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>175.850006</td>\n",
       "      <td>175.850006</td>\n",
       "      <td>27436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>174.279999</td>\n",
       "      <td>175.149994</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.970001</td>\n",
       "      <td>174.970001</td>\n",
       "      <td>25886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4762</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>174.320007</td>\n",
       "      <td>174.910004</td>\n",
       "      <td>172.919998</td>\n",
       "      <td>173.149994</td>\n",
       "      <td>173.149994</td>\n",
       "      <td>28215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>173.210007</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>172.729996</td>\n",
       "      <td>174.869995</td>\n",
       "      <td>174.869995</td>\n",
       "      <td>27835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>173.710007</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>173.169998</td>\n",
       "      <td>174.330002</td>\n",
       "      <td>174.330002</td>\n",
       "      <td>17070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>174.160004</td>\n",
       "      <td>175.869995</td>\n",
       "      <td>173.949997</td>\n",
       "      <td>174.229996</td>\n",
       "      <td>174.229996</td>\n",
       "      <td>21873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>171.580002</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>171.380005</td>\n",
       "      <td>172.970001</td>\n",
       "      <td>172.970001</td>\n",
       "      <td>18913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>171.800003</td>\n",
       "      <td>172.369995</td>\n",
       "      <td>170.300003</td>\n",
       "      <td>171.059998</td>\n",
       "      <td>171.059998</td>\n",
       "      <td>17249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>171.190002</td>\n",
       "      <td>173.320007</td>\n",
       "      <td>170.990005</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>26114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>169.710007</td>\n",
       "      <td>171.440002</td>\n",
       "      <td>169.490005</td>\n",
       "      <td>170.929993</td>\n",
       "      <td>170.929993</td>\n",
       "      <td>18972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>171.250000</td>\n",
       "      <td>171.699997</td>\n",
       "      <td>169.750000</td>\n",
       "      <td>170.419998</td>\n",
       "      <td>170.419998</td>\n",
       "      <td>24626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>169.710007</td>\n",
       "      <td>171.259995</td>\n",
       "      <td>169.380005</td>\n",
       "      <td>170.800003</td>\n",
       "      <td>170.800003</td>\n",
       "      <td>21835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>171.389999</td>\n",
       "      <td>172.479996</td>\n",
       "      <td>169.919998</td>\n",
       "      <td>170.179993</td>\n",
       "      <td>170.179993</td>\n",
       "      <td>22490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>170.100006</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>169.699997</td>\n",
       "      <td>170.889999</td>\n",
       "      <td>170.889999</td>\n",
       "      <td>22283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>171.050003</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>169.250000</td>\n",
       "      <td>169.429993</td>\n",
       "      <td>169.429993</td>\n",
       "      <td>20993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>168.990005</td>\n",
       "      <td>170.660004</td>\n",
       "      <td>168.419998</td>\n",
       "      <td>170.410004</td>\n",
       "      <td>170.410004</td>\n",
       "      <td>23820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000-05-04</td>\n",
       "      <td>4.111607</td>\n",
       "      <td>4.116071</td>\n",
       "      <td>3.948661</td>\n",
       "      <td>3.953125</td>\n",
       "      <td>2.635956</td>\n",
       "      <td>99878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>4.247768</td>\n",
       "      <td>4.330357</td>\n",
       "      <td>3.986607</td>\n",
       "      <td>4.109375</td>\n",
       "      <td>2.740144</td>\n",
       "      <td>122449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000-05-02</td>\n",
       "      <td>4.401786</td>\n",
       "      <td>4.508929</td>\n",
       "      <td>4.196429</td>\n",
       "      <td>4.209821</td>\n",
       "      <td>2.807121</td>\n",
       "      <td>59108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>4.459821</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>4.352679</td>\n",
       "      <td>4.439732</td>\n",
       "      <td>2.960427</td>\n",
       "      <td>56548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2000-04-28</td>\n",
       "      <td>4.540179</td>\n",
       "      <td>4.553571</td>\n",
       "      <td>4.332589</td>\n",
       "      <td>4.430804</td>\n",
       "      <td>2.954473</td>\n",
       "      <td>62395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>4.185268</td>\n",
       "      <td>4.535714</td>\n",
       "      <td>4.163504</td>\n",
       "      <td>4.526786</td>\n",
       "      <td>3.018473</td>\n",
       "      <td>81650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2000-04-26</td>\n",
       "      <td>4.522321</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>4.332589</td>\n",
       "      <td>2.888983</td>\n",
       "      <td>91728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2000-04-25</td>\n",
       "      <td>4.361607</td>\n",
       "      <td>4.598214</td>\n",
       "      <td>4.359375</td>\n",
       "      <td>4.582589</td>\n",
       "      <td>3.055684</td>\n",
       "      <td>97910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2000-04-24</td>\n",
       "      <td>4.107143</td>\n",
       "      <td>4.303571</td>\n",
       "      <td>4.098214</td>\n",
       "      <td>4.303571</td>\n",
       "      <td>2.869634</td>\n",
       "      <td>110905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000-04-20</td>\n",
       "      <td>4.417411</td>\n",
       "      <td>4.455357</td>\n",
       "      <td>4.180804</td>\n",
       "      <td>4.245536</td>\n",
       "      <td>2.830937</td>\n",
       "      <td>180530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-04-19</td>\n",
       "      <td>4.506696</td>\n",
       "      <td>4.651786</td>\n",
       "      <td>4.276786</td>\n",
       "      <td>4.325893</td>\n",
       "      <td>2.884518</td>\n",
       "      <td>130037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-04-18</td>\n",
       "      <td>4.410714</td>\n",
       "      <td>4.531250</td>\n",
       "      <td>4.263393</td>\n",
       "      <td>4.531250</td>\n",
       "      <td>3.021451</td>\n",
       "      <td>97731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-04-17</td>\n",
       "      <td>3.910714</td>\n",
       "      <td>4.426339</td>\n",
       "      <td>3.895089</td>\n",
       "      <td>4.424107</td>\n",
       "      <td>2.950008</td>\n",
       "      <td>102390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>3.904018</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>3.892857</td>\n",
       "      <td>3.995536</td>\n",
       "      <td>2.664236</td>\n",
       "      <td>166905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-04-13</td>\n",
       "      <td>3.982143</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4.064732</td>\n",
       "      <td>2.710376</td>\n",
       "      <td>132456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-04-12</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.745536</td>\n",
       "      <td>3.901786</td>\n",
       "      <td>2.601723</td>\n",
       "      <td>235284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-04-11</td>\n",
       "      <td>4.410714</td>\n",
       "      <td>4.459821</td>\n",
       "      <td>4.216518</td>\n",
       "      <td>4.265625</td>\n",
       "      <td>2.844331</td>\n",
       "      <td>135455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-04-10</td>\n",
       "      <td>4.703125</td>\n",
       "      <td>4.741071</td>\n",
       "      <td>4.455357</td>\n",
       "      <td>4.464286</td>\n",
       "      <td>2.976799</td>\n",
       "      <td>53065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-04-07</td>\n",
       "      <td>4.544643</td>\n",
       "      <td>4.709821</td>\n",
       "      <td>4.482143</td>\n",
       "      <td>4.705357</td>\n",
       "      <td>3.137547</td>\n",
       "      <td>60608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>4.665179</td>\n",
       "      <td>4.803571</td>\n",
       "      <td>4.401786</td>\n",
       "      <td>4.470982</td>\n",
       "      <td>2.981264</td>\n",
       "      <td>64906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-04-05</td>\n",
       "      <td>4.516739</td>\n",
       "      <td>4.745536</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>4.656250</td>\n",
       "      <td>3.104802</td>\n",
       "      <td>114416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>4.736607</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>4.169643</td>\n",
       "      <td>4.546875</td>\n",
       "      <td>3.031870</td>\n",
       "      <td>165082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-04-03</td>\n",
       "      <td>4.839286</td>\n",
       "      <td>4.982143</td>\n",
       "      <td>4.622768</td>\n",
       "      <td>4.761161</td>\n",
       "      <td>3.174755</td>\n",
       "      <td>82140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>4.551339</td>\n",
       "      <td>4.901786</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.850446</td>\n",
       "      <td>3.234293</td>\n",
       "      <td>101158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-03-30</td>\n",
       "      <td>4.770089</td>\n",
       "      <td>4.917411</td>\n",
       "      <td>4.479911</td>\n",
       "      <td>4.491071</td>\n",
       "      <td>2.994658</td>\n",
       "      <td>103600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>4.977679</td>\n",
       "      <td>4.979911</td>\n",
       "      <td>4.779575</td>\n",
       "      <td>4.854911</td>\n",
       "      <td>3.237269</td>\n",
       "      <td>59959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>4.901786</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>4.897321</td>\n",
       "      <td>4.968750</td>\n",
       "      <td>3.313178</td>\n",
       "      <td>50741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-27</td>\n",
       "      <td>4.915179</td>\n",
       "      <td>5.169643</td>\n",
       "      <td>4.888393</td>\n",
       "      <td>4.984375</td>\n",
       "      <td>3.323597</td>\n",
       "      <td>69795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>5.087054</td>\n",
       "      <td>5.140625</td>\n",
       "      <td>4.839286</td>\n",
       "      <td>4.953125</td>\n",
       "      <td>3.302759</td>\n",
       "      <td>111728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-03-23</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>5.370536</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.046875</td>\n",
       "      <td>3.365272</td>\n",
       "      <td>140641200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4779 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "4778  2019-03-22  195.339996  197.690002  190.779999  191.050003  191.050003   \n",
       "4777  2019-03-21  190.020004  196.330002  189.809998  195.089996  195.089996   \n",
       "4776  2019-03-20  186.229996  189.490005  184.729996  188.160004  188.160004   \n",
       "4775  2019-03-19  188.350006  188.990005  185.919998  186.529999  186.529999   \n",
       "4774  2019-03-18  185.800003  188.389999  185.789993  188.020004  188.020004   \n",
       "4773  2019-03-15  184.850006  187.330002  183.740005  186.119995  186.119995   \n",
       "4772  2019-03-14  183.899994  184.100006  182.559998  183.729996  183.729996   \n",
       "4771  2019-03-13  182.250000  183.300003  180.919998  181.710007  181.710007   \n",
       "4770  2019-03-12  180.000000  182.669998  179.369995  180.910004  180.910004   \n",
       "4769  2019-03-11  175.490005  179.119995  175.350006  178.899994  178.899994   \n",
       "4768  2019-03-08  170.320007  173.070007  169.500000  172.910004  172.910004   \n",
       "4767  2019-03-07  173.869995  174.440002  172.020004  172.500000  172.500000   \n",
       "4766  2019-03-06  174.669998  175.490005  173.940002  174.520004  174.520004   \n",
       "4765  2019-03-05  175.940002  176.000000  174.539993  175.529999  175.529999   \n",
       "4764  2019-03-04  175.690002  177.750000  173.970001  175.850006  175.850006   \n",
       "4763  2019-03-01  174.279999  175.149994  172.889999  174.970001  174.970001   \n",
       "4762  2019-02-28  174.320007  174.910004  172.919998  173.149994  173.149994   \n",
       "4761  2019-02-27  173.210007  175.000000  172.729996  174.869995  174.869995   \n",
       "4760  2019-02-26  173.710007  175.300003  173.169998  174.330002  174.330002   \n",
       "4759  2019-02-25  174.160004  175.869995  173.949997  174.229996  174.229996   \n",
       "4758  2019-02-22  171.580002  173.000000  171.380005  172.970001  172.970001   \n",
       "4757  2019-02-21  171.800003  172.369995  170.300003  171.059998  171.059998   \n",
       "4756  2019-02-20  171.190002  173.320007  170.990005  172.029999  172.029999   \n",
       "4755  2019-02-19  169.710007  171.440002  169.490005  170.929993  170.929993   \n",
       "4754  2019-02-15  171.250000  171.699997  169.750000  170.419998  170.419998   \n",
       "4753  2019-02-14  169.710007  171.259995  169.380005  170.800003  170.800003   \n",
       "4752  2019-02-13  171.389999  172.479996  169.919998  170.179993  170.179993   \n",
       "4751  2019-02-12  170.100006  171.000000  169.699997  170.889999  170.889999   \n",
       "4750  2019-02-11  171.050003  171.210007  169.250000  169.429993  169.429993   \n",
       "4749  2019-02-08  168.990005  170.660004  168.419998  170.410004  170.410004   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "29    2000-05-04    4.111607    4.116071    3.948661    3.953125    2.635956   \n",
       "28    2000-05-03    4.247768    4.330357    3.986607    4.109375    2.740144   \n",
       "27    2000-05-02    4.401786    4.508929    4.196429    4.209821    2.807121   \n",
       "26    2000-05-01    4.459821    4.468750    4.352679    4.439732    2.960427   \n",
       "25    2000-04-28    4.540179    4.553571    4.332589    4.430804    2.954473   \n",
       "24    2000-04-27    4.185268    4.535714    4.163504    4.526786    3.018473   \n",
       "23    2000-04-26    4.522321    4.571429    4.285714    4.332589    2.888983   \n",
       "22    2000-04-25    4.361607    4.598214    4.359375    4.582589    3.055684   \n",
       "21    2000-04-24    4.107143    4.303571    4.098214    4.303571    2.869634   \n",
       "20    2000-04-20    4.417411    4.455357    4.180804    4.245536    2.830937   \n",
       "19    2000-04-19    4.506696    4.651786    4.276786    4.325893    2.884518   \n",
       "18    2000-04-18    4.410714    4.531250    4.263393    4.531250    3.021451   \n",
       "17    2000-04-17    3.910714    4.426339    3.895089    4.424107    2.950008   \n",
       "16    2000-04-14    3.904018    4.214286    3.892857    3.995536    2.664236   \n",
       "15    2000-04-13    3.982143    4.285714    3.875000    4.064732    2.710376   \n",
       "14    2000-04-12    4.250000    4.250000    3.745536    3.901786    2.601723   \n",
       "13    2000-04-11    4.410714    4.459821    4.216518    4.265625    2.844331   \n",
       "12    2000-04-10    4.703125    4.741071    4.455357    4.464286    2.976799   \n",
       "11    2000-04-07    4.544643    4.709821    4.482143    4.705357    3.137547   \n",
       "10    2000-04-06    4.665179    4.803571    4.401786    4.470982    2.981264   \n",
       "9     2000-04-05    4.516739    4.745536    4.428571    4.656250    3.104802   \n",
       "8     2000-04-04    4.736607    4.750000    4.169643    4.546875    3.031870   \n",
       "7     2000-04-03    4.839286    4.982143    4.622768    4.761161    3.174755   \n",
       "6     2000-03-31    4.551339    4.901786    4.500000    4.850446    3.234293   \n",
       "5     2000-03-30    4.770089    4.917411    4.479911    4.491071    2.994658   \n",
       "4     2000-03-29    4.977679    4.979911    4.779575    4.854911    3.237269   \n",
       "3     2000-03-28    4.901786    5.071429    4.897321    4.968750    3.313178   \n",
       "2     2000-03-27    4.915179    5.169643    4.888393    4.984375    3.323597   \n",
       "1     2000-03-24    5.087054    5.140625    4.839286    4.953125    3.302759   \n",
       "0     2000-03-23    5.071429    5.370536    5.000000    5.046875    3.365272   \n",
       "\n",
       "         Volume  \n",
       "4778   42359300  \n",
       "4777   51034200  \n",
       "4776   31035200  \n",
       "4775   31646400  \n",
       "4774   26219800  \n",
       "4773   39042900  \n",
       "4772   23579500  \n",
       "4771   31032500  \n",
       "4770   32467600  \n",
       "4769   32011000  \n",
       "4768   23999400  \n",
       "4767   24796400  \n",
       "4766   20810400  \n",
       "4765   19737400  \n",
       "4764   27436200  \n",
       "4763   25886200  \n",
       "4762   28215400  \n",
       "4761   27835400  \n",
       "4760   17070200  \n",
       "4759   21873400  \n",
       "4758   18913200  \n",
       "4757   17249700  \n",
       "4756   26114400  \n",
       "4755   18972800  \n",
       "4754   24626800  \n",
       "4753   21835700  \n",
       "4752   22490200  \n",
       "4751   22283500  \n",
       "4750   20993400  \n",
       "4749   23820000  \n",
       "...         ...  \n",
       "29     99878800  \n",
       "28    122449600  \n",
       "27     59108000  \n",
       "26     56548800  \n",
       "25     62395200  \n",
       "24     81650800  \n",
       "23     91728000  \n",
       "22     97910400  \n",
       "21    110905200  \n",
       "20    180530000  \n",
       "19    130037600  \n",
       "18     97731200  \n",
       "17    102390400  \n",
       "16    166905200  \n",
       "15    132456800  \n",
       "14    235284000  \n",
       "13    135455600  \n",
       "12     53065600  \n",
       "11     60608800  \n",
       "10     64906800  \n",
       "9     114416400  \n",
       "8     165082400  \n",
       "7      82140800  \n",
       "6     101158400  \n",
       "5     103600000  \n",
       "4      59959200  \n",
       "3      50741600  \n",
       "2      69795600  \n",
       "1     111728400  \n",
       "0     140641200  \n",
       "\n",
       "[4779 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>119.500000</td>\n",
       "      <td>119.589996</td>\n",
       "      <td>117.040001</td>\n",
       "      <td>117.050003</td>\n",
       "      <td>117.050003</td>\n",
       "      <td>33619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>117.139999</td>\n",
       "      <td>120.820000</td>\n",
       "      <td>117.089996</td>\n",
       "      <td>120.220001</td>\n",
       "      <td>120.220001</td>\n",
       "      <td>29854400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>117.389999</td>\n",
       "      <td>118.750000</td>\n",
       "      <td>116.709999</td>\n",
       "      <td>117.519997</td>\n",
       "      <td>117.519997</td>\n",
       "      <td>28113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>118.089996</td>\n",
       "      <td>118.440002</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>117.650002</td>\n",
       "      <td>117.650002</td>\n",
       "      <td>37588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>116.169998</td>\n",
       "      <td>117.610001</td>\n",
       "      <td>116.050003</td>\n",
       "      <td>117.570000</td>\n",
       "      <td>117.570000</td>\n",
       "      <td>31207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>115.339996</td>\n",
       "      <td>117.250000</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>115.910004</td>\n",
       "      <td>115.910004</td>\n",
       "      <td>54681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>114.540001</td>\n",
       "      <td>115.199997</td>\n",
       "      <td>114.330002</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>114.589996</td>\n",
       "      <td>30763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>114.129997</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>113.779999</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>35513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>112.820000</td>\n",
       "      <td>113.989998</td>\n",
       "      <td>112.650002</td>\n",
       "      <td>113.620003</td>\n",
       "      <td>113.620003</td>\n",
       "      <td>26132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>110.989998</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>110.980003</td>\n",
       "      <td>112.830002</td>\n",
       "      <td>112.830002</td>\n",
       "      <td>26491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>109.160004</td>\n",
       "      <td>110.709999</td>\n",
       "      <td>108.800003</td>\n",
       "      <td>110.510002</td>\n",
       "      <td>110.510002</td>\n",
       "      <td>22818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>111.550003</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>110.389999</td>\n",
       "      <td>110.389999</td>\n",
       "      <td>25339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>111.870003</td>\n",
       "      <td>112.660004</td>\n",
       "      <td>111.430000</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>17687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>112.250000</td>\n",
       "      <td>112.389999</td>\n",
       "      <td>111.230003</td>\n",
       "      <td>111.699997</td>\n",
       "      <td>111.699997</td>\n",
       "      <td>19538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>113.019997</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>110.800003</td>\n",
       "      <td>112.260002</td>\n",
       "      <td>112.260002</td>\n",
       "      <td>26608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>112.889999</td>\n",
       "      <td>113.019997</td>\n",
       "      <td>111.669998</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>23501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4762</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>112.040001</td>\n",
       "      <td>112.879997</td>\n",
       "      <td>111.730003</td>\n",
       "      <td>112.029999</td>\n",
       "      <td>112.029999</td>\n",
       "      <td>29083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>111.690002</td>\n",
       "      <td>112.360001</td>\n",
       "      <td>110.879997</td>\n",
       "      <td>112.169998</td>\n",
       "      <td>112.169998</td>\n",
       "      <td>21487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>111.260002</td>\n",
       "      <td>113.239998</td>\n",
       "      <td>111.169998</td>\n",
       "      <td>112.360001</td>\n",
       "      <td>112.360001</td>\n",
       "      <td>21536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>111.760002</td>\n",
       "      <td>112.180000</td>\n",
       "      <td>111.260002</td>\n",
       "      <td>111.589996</td>\n",
       "      <td>111.589996</td>\n",
       "      <td>23750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>111.199997</td>\n",
       "      <td>109.820000</td>\n",
       "      <td>110.970001</td>\n",
       "      <td>110.970001</td>\n",
       "      <td>27763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>106.900002</td>\n",
       "      <td>109.480003</td>\n",
       "      <td>106.870003</td>\n",
       "      <td>109.410004</td>\n",
       "      <td>109.410004</td>\n",
       "      <td>29063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>107.860001</td>\n",
       "      <td>107.940002</td>\n",
       "      <td>106.290001</td>\n",
       "      <td>107.150002</td>\n",
       "      <td>107.150002</td>\n",
       "      <td>21607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>107.790001</td>\n",
       "      <td>108.660004</td>\n",
       "      <td>107.779999</td>\n",
       "      <td>108.169998</td>\n",
       "      <td>107.709999</td>\n",
       "      <td>18038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>107.910004</td>\n",
       "      <td>108.300003</td>\n",
       "      <td>107.360001</td>\n",
       "      <td>108.220001</td>\n",
       "      <td>107.759789</td>\n",
       "      <td>26606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>106.309998</td>\n",
       "      <td>107.290001</td>\n",
       "      <td>105.660004</td>\n",
       "      <td>106.900002</td>\n",
       "      <td>106.445404</td>\n",
       "      <td>21784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>107.779999</td>\n",
       "      <td>106.709999</td>\n",
       "      <td>106.809998</td>\n",
       "      <td>106.355782</td>\n",
       "      <td>18394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>106.139999</td>\n",
       "      <td>107.139999</td>\n",
       "      <td>105.480003</td>\n",
       "      <td>106.889999</td>\n",
       "      <td>106.435448</td>\n",
       "      <td>25056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>106.199997</td>\n",
       "      <td>106.580002</td>\n",
       "      <td>104.970001</td>\n",
       "      <td>105.250000</td>\n",
       "      <td>104.802422</td>\n",
       "      <td>18914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>104.389999</td>\n",
       "      <td>105.779999</td>\n",
       "      <td>104.260002</td>\n",
       "      <td>105.669998</td>\n",
       "      <td>105.220634</td>\n",
       "      <td>21461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000-05-04</td>\n",
       "      <td>35.156250</td>\n",
       "      <td>35.625000</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>35.218750</td>\n",
       "      <td>25.449842</td>\n",
       "      <td>43317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>35.187500</td>\n",
       "      <td>35.406250</td>\n",
       "      <td>34.406250</td>\n",
       "      <td>35.281250</td>\n",
       "      <td>25.495012</td>\n",
       "      <td>55354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000-05-02</td>\n",
       "      <td>36.406250</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>34.937500</td>\n",
       "      <td>25.246618</td>\n",
       "      <td>97716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>36.437500</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>35.843750</td>\n",
       "      <td>36.718750</td>\n",
       "      <td>26.533783</td>\n",
       "      <td>107811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2000-04-28</td>\n",
       "      <td>35.375000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>34.125000</td>\n",
       "      <td>34.875000</td>\n",
       "      <td>25.201454</td>\n",
       "      <td>78082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000-04-27</td>\n",
       "      <td>33.718750</td>\n",
       "      <td>34.968750</td>\n",
       "      <td>33.687500</td>\n",
       "      <td>34.906250</td>\n",
       "      <td>25.224031</td>\n",
       "      <td>77669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2000-04-26</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.562500</td>\n",
       "      <td>33.687500</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>24.569155</td>\n",
       "      <td>107091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2000-04-25</td>\n",
       "      <td>34.375000</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>33.812500</td>\n",
       "      <td>34.687500</td>\n",
       "      <td>25.065952</td>\n",
       "      <td>159517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2000-04-24</td>\n",
       "      <td>33.625000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>33.312500</td>\n",
       "      <td>24.072346</td>\n",
       "      <td>313645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000-04-20</td>\n",
       "      <td>39.312500</td>\n",
       "      <td>39.937500</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>39.468750</td>\n",
       "      <td>28.520998</td>\n",
       "      <td>52387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-04-19</td>\n",
       "      <td>40.718750</td>\n",
       "      <td>40.750000</td>\n",
       "      <td>39.062500</td>\n",
       "      <td>39.343750</td>\n",
       "      <td>28.430660</td>\n",
       "      <td>53715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-04-18</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>40.968750</td>\n",
       "      <td>37.937500</td>\n",
       "      <td>40.281250</td>\n",
       "      <td>29.108118</td>\n",
       "      <td>91794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-04-17</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>37.937500</td>\n",
       "      <td>27.414469</td>\n",
       "      <td>119772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>39.562500</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>37.062500</td>\n",
       "      <td>26.782173</td>\n",
       "      <td>151217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-04-13</td>\n",
       "      <td>40.437500</td>\n",
       "      <td>41.125000</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>39.625000</td>\n",
       "      <td>28.633890</td>\n",
       "      <td>94316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-04-12</td>\n",
       "      <td>41.062500</td>\n",
       "      <td>41.125000</td>\n",
       "      <td>39.375000</td>\n",
       "      <td>39.687500</td>\n",
       "      <td>28.679054</td>\n",
       "      <td>153003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-04-11</td>\n",
       "      <td>42.562500</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>41.750000</td>\n",
       "      <td>41.937500</td>\n",
       "      <td>30.304972</td>\n",
       "      <td>71961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-04-10</td>\n",
       "      <td>44.312500</td>\n",
       "      <td>44.312500</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>31.095318</td>\n",
       "      <td>60685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-04-07</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>44.687500</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>44.531250</td>\n",
       "      <td>32.179268</td>\n",
       "      <td>82613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-04-06</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>42.632801</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>31.072748</td>\n",
       "      <td>66421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-04-05</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>42.937500</td>\n",
       "      <td>43.187500</td>\n",
       "      <td>31.208242</td>\n",
       "      <td>82887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>45.781250</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>42.468750</td>\n",
       "      <td>44.281250</td>\n",
       "      <td>31.998610</td>\n",
       "      <td>181244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-04-03</td>\n",
       "      <td>47.218750</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.437500</td>\n",
       "      <td>32.834122</td>\n",
       "      <td>260118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>54.125000</td>\n",
       "      <td>52.062500</td>\n",
       "      <td>53.125000</td>\n",
       "      <td>38.389305</td>\n",
       "      <td>64281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-03-30</td>\n",
       "      <td>53.093750</td>\n",
       "      <td>54.312500</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>51.687500</td>\n",
       "      <td>37.350536</td>\n",
       "      <td>64178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>52.593750</td>\n",
       "      <td>54.468750</td>\n",
       "      <td>52.562500</td>\n",
       "      <td>53.593750</td>\n",
       "      <td>38.728024</td>\n",
       "      <td>64363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>51.812500</td>\n",
       "      <td>53.718750</td>\n",
       "      <td>51.187500</td>\n",
       "      <td>52.156250</td>\n",
       "      <td>37.689251</td>\n",
       "      <td>81114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-27</td>\n",
       "      <td>53.882801</td>\n",
       "      <td>54.125000</td>\n",
       "      <td>51.968750</td>\n",
       "      <td>52.031250</td>\n",
       "      <td>37.598930</td>\n",
       "      <td>111434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>56.312500</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>54.781250</td>\n",
       "      <td>55.843750</td>\n",
       "      <td>40.353931</td>\n",
       "      <td>112196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-03-23</td>\n",
       "      <td>53.406250</td>\n",
       "      <td>56.437500</td>\n",
       "      <td>53.312500</td>\n",
       "      <td>55.937500</td>\n",
       "      <td>40.421658</td>\n",
       "      <td>148224000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4779 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "4778  2019-03-22  119.500000  119.589996  117.040001  117.050003  117.050003   \n",
       "4777  2019-03-21  117.139999  120.820000  117.089996  120.220001  120.220001   \n",
       "4776  2019-03-20  117.389999  118.750000  116.709999  117.519997  117.519997   \n",
       "4775  2019-03-19  118.089996  118.440002  116.989998  117.650002  117.650002   \n",
       "4774  2019-03-18  116.169998  117.610001  116.050003  117.570000  117.570000   \n",
       "4773  2019-03-15  115.339996  117.250000  114.589996  115.910004  115.910004   \n",
       "4772  2019-03-14  114.540001  115.199997  114.330002  114.589996  114.589996   \n",
       "4771  2019-03-13  114.129997  115.000000  113.779999  114.500000  114.500000   \n",
       "4770  2019-03-12  112.820000  113.989998  112.650002  113.620003  113.620003   \n",
       "4769  2019-03-11  110.989998  112.949997  110.980003  112.830002  112.830002   \n",
       "4768  2019-03-08  109.160004  110.709999  108.800003  110.510002  110.510002   \n",
       "4767  2019-03-07  111.400002  111.550003  109.870003  110.389999  110.389999   \n",
       "4766  2019-03-06  111.870003  112.660004  111.430000  111.750000  111.750000   \n",
       "4765  2019-03-05  112.250000  112.389999  111.230003  111.699997  111.699997   \n",
       "4764  2019-03-04  113.019997  113.250000  110.800003  112.260002  112.260002   \n",
       "4763  2019-03-01  112.889999  113.019997  111.669998  112.529999  112.529999   \n",
       "4762  2019-02-28  112.040001  112.879997  111.730003  112.029999  112.029999   \n",
       "4761  2019-02-27  111.690002  112.360001  110.879997  112.169998  112.169998   \n",
       "4760  2019-02-26  111.260002  113.239998  111.169998  112.360001  112.360001   \n",
       "4759  2019-02-25  111.760002  112.180000  111.260002  111.589996  111.589996   \n",
       "4758  2019-02-22  110.050003  111.199997  109.820000  110.970001  110.970001   \n",
       "4757  2019-02-21  106.900002  109.480003  106.870003  109.410004  109.410004   \n",
       "4756  2019-02-20  107.860001  107.940002  106.290001  107.150002  107.150002   \n",
       "4755  2019-02-19  107.790001  108.660004  107.779999  108.169998  107.709999   \n",
       "4754  2019-02-15  107.910004  108.300003  107.360001  108.220001  107.759789   \n",
       "4753  2019-02-14  106.309998  107.290001  105.660004  106.900002  106.445404   \n",
       "4752  2019-02-13  107.500000  107.779999  106.709999  106.809998  106.355782   \n",
       "4751  2019-02-12  106.139999  107.139999  105.480003  106.889999  106.435448   \n",
       "4750  2019-02-11  106.199997  106.580002  104.970001  105.250000  104.802422   \n",
       "4749  2019-02-08  104.389999  105.779999  104.260002  105.669998  105.220634   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "29    2000-05-04   35.156250   35.625000   34.656250   35.218750   25.449842   \n",
       "28    2000-05-03   35.187500   35.406250   34.406250   35.281250   25.495012   \n",
       "27    2000-05-02   36.406250   36.750000   34.750000   34.937500   25.246618   \n",
       "26    2000-05-01   36.437500   37.000000   35.843750   36.718750   26.533783   \n",
       "25    2000-04-28   35.375000   35.500000   34.125000   34.875000   25.201454   \n",
       "24    2000-04-27   33.718750   34.968750   33.687500   34.906250   25.224031   \n",
       "23    2000-04-26   35.000000   35.562500   33.687500   34.000000   24.569155   \n",
       "22    2000-04-25   34.375000   34.750000   33.812500   34.687500   25.065952   \n",
       "21    2000-04-24   33.625000   34.000000   32.500000   33.312500   24.072346   \n",
       "20    2000-04-20   39.312500   39.937500   38.750000   39.468750   28.520998   \n",
       "19    2000-04-19   40.718750   40.750000   39.062500   39.343750   28.430660   \n",
       "18    2000-04-18   38.250000   40.968750   37.937500   40.281250   29.108118   \n",
       "17    2000-04-17   37.125000   38.000000   36.500000   37.937500   27.414469   \n",
       "16    2000-04-14   39.562500   39.750000   36.625000   37.062500   26.782173   \n",
       "15    2000-04-13   40.437500   41.125000   39.500000   39.625000   28.633890   \n",
       "14    2000-04-12   41.062500   41.125000   39.375000   39.687500   28.679054   \n",
       "13    2000-04-11   42.562500   43.031250   41.750000   41.937500   30.304972   \n",
       "12    2000-04-10   44.312500   44.312500   43.000000   43.031250   31.095318   \n",
       "11    2000-04-07   43.500000   44.687500   42.500000   44.531250   32.179268   \n",
       "10    2000-04-06   43.937500   44.000000   42.632801   43.000000   31.072748   \n",
       "9     2000-04-05   44.125000   44.250000   42.937500   43.187500   31.208242   \n",
       "8     2000-04-04   45.781250   46.000000   42.468750   44.281250   31.998610   \n",
       "7     2000-04-03   47.218750   48.250000   45.000000   45.437500   32.834122   \n",
       "6     2000-03-31   53.000000   54.125000   52.062500   53.125000   38.389305   \n",
       "5     2000-03-30   53.093750   54.312500   51.250000   51.687500   37.350536   \n",
       "4     2000-03-29   52.593750   54.468750   52.562500   53.593750   38.728024   \n",
       "3     2000-03-28   51.812500   53.718750   51.187500   52.156250   37.689251   \n",
       "2     2000-03-27   53.882801   54.125000   51.968750   52.031250   37.598930   \n",
       "1     2000-03-24   56.312500   57.500000   54.781250   55.843750   40.353931   \n",
       "0     2000-03-23   53.406250   56.437500   53.312500   55.937500   40.421658   \n",
       "\n",
       "         Volume  \n",
       "4778   33619100  \n",
       "4777   29854400  \n",
       "4776   28113300  \n",
       "4775   37588700  \n",
       "4774   31207600  \n",
       "4773   54681100  \n",
       "4772   30763400  \n",
       "4771   35513800  \n",
       "4770   26132700  \n",
       "4769   26491600  \n",
       "4768   22818400  \n",
       "4767   25339000  \n",
       "4766   17687000  \n",
       "4765   19538300  \n",
       "4764   26608000  \n",
       "4763   23501200  \n",
       "4762   29083900  \n",
       "4761   21487100  \n",
       "4760   21536700  \n",
       "4759   23750600  \n",
       "4758   27763200  \n",
       "4757   29063200  \n",
       "4756   21607700  \n",
       "4755   18038500  \n",
       "4754   26606900  \n",
       "4753   21784700  \n",
       "4752   18394900  \n",
       "4751   25056600  \n",
       "4750   18914100  \n",
       "4749   21461100  \n",
       "...         ...  \n",
       "29     43317200  \n",
       "28     55354800  \n",
       "27     97716200  \n",
       "26    107811000  \n",
       "25     78082600  \n",
       "24     77669800  \n",
       "23    107091200  \n",
       "22    159517400  \n",
       "21    313645800  \n",
       "20     52387400  \n",
       "19     53715400  \n",
       "18     91794600  \n",
       "17    119772200  \n",
       "16    151217800  \n",
       "15     94316200  \n",
       "14    153003800  \n",
       "13     71961800  \n",
       "12     60685400  \n",
       "11     82613600  \n",
       "10     66421400  \n",
       "9      82887600  \n",
       "8     181244400  \n",
       "7     260118200  \n",
       "6      64281400  \n",
       "5      64178400  \n",
       "4      64363800  \n",
       "3      81114400  \n",
       "2     111434000  \n",
       "1     112196800  \n",
       "0     148224000  \n",
       "\n",
       "[4779 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plt_data(data):\n",
    "    \"\"\"Строит pyplot график.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas.core.frame.DataFrame\n",
    "               Данные в формате сайта finance.yahoo.com\n",
    "    \"\"\"\n",
    "    close_price = data.loc[:, 'Adj Close'].tolist()\n",
    "    plt.plot(close_price)\n",
    "    plt.show()\n",
    "\n",
    "plt_data(aapl_data)\n",
    "plt_data(msft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(finance_data, company_names, window=60, forecast=1, step=1, test_size=0.10, random_state=666):\n",
    "    '''kakacode\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        finance_data : pandas.core.frame.DataFrame\n",
    "                       Данные в формате сайта finance.yahoo.com\n",
    "        window       : int                             (default 30)\n",
    "                       Размер окна \n",
    "        forecast     : int                              (default 1)\n",
    "                       Промежуток через который будем пердсказывать\n",
    "        step         : int                              (default 1)\n",
    "                       Ifu\n",
    "                       \n",
    "                       \n",
    "    '''\n",
    "    X, ys = [], []\n",
    "    \n",
    "    data_adj_close = [df.loc[:, 'Adj Close'].tolist() for df in finance_data]\n",
    "    #data_open = [df.loc[:, 'Open'].tolist() for df in finance_data]\n",
    "    #data_high = [df.loc[:, 'High'].tolist() for df in finance_data]\n",
    "    #data_low = [df.loc[:, 'Low'].tolist() for df in finance_data]\n",
    "    #data_close = [df.loc[:, 'Close'].tolist() for df in finance_data]\n",
    "    #data_volume = [df.loc[:, 'Volume'].tolist() for df in finance_data]\n",
    "    \n",
    "    for i in range(0, len(data_adj_close[0]) - forecast - window, step):\n",
    "        x, y = [], []\n",
    "        \n",
    "        for data in data_adj_close:\n",
    "            x_i = data[i:i+window]\n",
    "            y_i = data[i+window+forecast]  \n",
    "\n",
    "            last_close = x_i[window-1]\n",
    "            next_close = y_i\n",
    "            if last_close < next_close:\n",
    "                y_i = [1]\n",
    "            else:\n",
    "                y_i = [0] \n",
    "            x_i = (np.array(x_i) - np.mean(x_i)) / np.std(x_i)\n",
    "            x.append(x_i)\n",
    "            y.append(y_i)\n",
    "        \n",
    "        x = [[x[c_i][w_i] for c_i in range(len(data_adj_close))] for w_i in range(window)]\n",
    "        X.append(x)\n",
    "        ys.append(y)\n",
    "    \n",
    "    X, ys = np.array(X), np.array(ys)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, ys, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    y_train = {company_names[c_i]:np.array([[y_train[i][c_i][w_i] for w_i in range(1)] for i in range(len(y_train))]) for c_i in range(len(company_names))} \n",
    "    y_test = {company_names[c_i]:np.array([[y_test[i][c_i][w_i] for w_i in range(1)] for i in range(len(y_test))]) for c_i in range(len(company_names))} \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#company_names = [\"aapl\"]\n",
    "#company_datas = [aapl_data]\n",
    "\n",
    "company_names =  [\"aapl\", \"msft\"]  \n",
    "company_datas =  [aapl_data, msft_data] \n",
    "X_train, X_test, y_train, y_test = create_windows(company_datas, company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(input_model):\n",
    "    base = Dense(64, activity_regularizer=regularizers.l2(0.01))(input_model)\n",
    "    base = BatchNormalization()(base)\n",
    "    base = LeakyReLU()(base)\n",
    "    base = Dropout(0.3)(base)\n",
    "    base = Flatten()(base)\n",
    "    \n",
    "    return base\n",
    "\n",
    "def create_lstm(input_model):\n",
    "    base = LSTM(30, return_sequences=True)(input_model)\n",
    "    base = LSTM(30)(base)\n",
    "    base = Dropout(0.3)(base)\n",
    "    base = BatchNormalization()(base)\n",
    "    base = LeakyReLU()(base)\n",
    "    \n",
    "    return base\n",
    "\n",
    "def create_cnn(input_model):\n",
    "    base = Conv1D(60, 2, activation='relu')(input_model)\n",
    "    base = Conv1D(60, 2, activation='relu')(base)\n",
    "    base = MaxPooling1D(2)(base)\n",
    "    base = Conv1D(30, 1, activation='relu')(base)\n",
    "    base = Conv1D(30, 1, activation='relu')(base)\n",
    "    base = GlobalAveragePooling1D()(base)\n",
    "    base = Dropout(0.3)(base)\n",
    "    \n",
    "    return base\n",
    "\n",
    "\n",
    "def create_model(company_names, input_day = 60):\n",
    "    K.clear_session()\n",
    "    \n",
    "    input_model = Input(name=\"input\", shape=(input_day, len(company_names), ))\n",
    "    base = create_lstm(input_model)\n",
    "    \n",
    "    outputs = []\n",
    "    loss_weights={}\n",
    "    loss={}\n",
    "    for i in range(len(company_names)):\n",
    "        x = Dense(32, activity_regularizer=regularizers.l2(0.01))(base)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('sigmoid', name=company_names[i])(x)\n",
    "        \n",
    "        outputs.append(x)\n",
    "        loss_weights[company_names[i]] = 1.0\n",
    "        loss[company_names[i]] = 'binary_crossentropy'\n",
    "    \n",
    "    model = Model(input=input_model, outputs=outputs)\n",
    "    \n",
    "    opt = Adam(lr=0.00025)\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=loss,\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3396 samples, validate on 850 samples\n",
      "Epoch 1/100\n",
      "3396/3396 [==============================] - 3s 779us/step - loss: 36.6257 - aapl_loss: 0.7775 - msft_loss: 0.7535 - aapl_acc: 0.5038 - msft_acc: 0.5018 - val_loss: 25.3067 - val_aapl_loss: 0.7342 - val_msft_loss: 0.7182 - val_aapl_acc: 0.5294 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 25.30670, saving model to test.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_aapl_loss,val_msft_loss,val_aapl_acc,val_msft_acc,loss,aapl_loss,msft_loss,aapl_acc,msft_acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "3396/3396 [==============================] - 1s 297us/step - loss: 36.5291 - aapl_loss: 0.7916 - msft_loss: 0.7545 - aapl_acc: 0.4879 - msft_acc: 0.5050 - val_loss: 25.2640 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7168 - val_aapl_acc: 0.5271 - val_msft_acc: 0.4965\n",
      "\n",
      "Epoch 00002: val_loss improved from 25.30670 to 25.26403, saving model to test.hdf5\n",
      "Epoch 3/100\n",
      "3396/3396 [==============================] - 1s 292us/step - loss: 36.5852 - aapl_loss: 0.7814 - msft_loss: 0.7559 - aapl_acc: 0.5032 - msft_acc: 0.5018 - val_loss: 25.3108 - val_aapl_loss: 0.7340 - val_msft_loss: 0.7157 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 25.26403\n",
      "Epoch 4/100\n",
      "3396/3396 [==============================] - 1s 323us/step - loss: 36.7232 - aapl_loss: 0.7941 - msft_loss: 0.7531 - aapl_acc: 0.4920 - msft_acc: 0.5018 - val_loss: 25.2068 - val_aapl_loss: 0.7337 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5271 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00004: val_loss improved from 25.26403 to 25.20680, saving model to test.hdf5\n",
      "Epoch 5/100\n",
      "3396/3396 [==============================] - 1s 296us/step - loss: 36.4906 - aapl_loss: 0.7893 - msft_loss: 0.7564 - aapl_acc: 0.4920 - msft_acc: 0.4947 - val_loss: 25.2344 - val_aapl_loss: 0.7336 - val_msft_loss: 0.7156 - val_aapl_acc: 0.5294 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 25.20680\n",
      "Epoch 6/100\n",
      "3396/3396 [==============================] - 1s 313us/step - loss: 36.5880 - aapl_loss: 0.7842 - msft_loss: 0.7601 - aapl_acc: 0.5027 - msft_acc: 0.5100 - val_loss: 25.1575 - val_aapl_loss: 0.7334 - val_msft_loss: 0.7151 - val_aapl_acc: 0.5271 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00006: val_loss improved from 25.20680 to 25.15754, saving model to test.hdf5\n",
      "Epoch 7/100\n",
      "3396/3396 [==============================] - 1s 359us/step - loss: 36.4143 - aapl_loss: 0.7911 - msft_loss: 0.7601 - aapl_acc: 0.4950 - msft_acc: 0.4944 - val_loss: 25.1239 - val_aapl_loss: 0.7333 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5271 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00007: val_loss improved from 25.15754 to 25.12388, saving model to test.hdf5\n",
      "Epoch 8/100\n",
      "3396/3396 [==============================] - 1s 335us/step - loss: 36.3685 - aapl_loss: 0.7860 - msft_loss: 0.7592 - aapl_acc: 0.5003 - msft_acc: 0.5009 - val_loss: 25.0636 - val_aapl_loss: 0.7334 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5271 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00008: val_loss improved from 25.12388 to 25.06360, saving model to test.hdf5\n",
      "Epoch 9/100\n",
      "3396/3396 [==============================] - 1s 296us/step - loss: 36.4079 - aapl_loss: 0.7994 - msft_loss: 0.7531 - aapl_acc: 0.4909 - msft_acc: 0.4988 - val_loss: 25.0209 - val_aapl_loss: 0.7335 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5259 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00009: val_loss improved from 25.06360 to 25.02086, saving model to test.hdf5\n",
      "Epoch 10/100\n",
      "3396/3396 [==============================] - 1s 306us/step - loss: 36.3441 - aapl_loss: 0.7872 - msft_loss: 0.7612 - aapl_acc: 0.5065 - msft_acc: 0.4865 - val_loss: 24.9537 - val_aapl_loss: 0.7337 - val_msft_loss: 0.7163 - val_aapl_acc: 0.5271 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00010: val_loss improved from 25.02086 to 24.95366, saving model to test.hdf5\n",
      "Epoch 11/100\n",
      "3396/3396 [==============================] - 1s 305us/step - loss: 36.3531 - aapl_loss: 0.7888 - msft_loss: 0.7588 - aapl_acc: 0.4994 - msft_acc: 0.5006 - val_loss: 24.8768 - val_aapl_loss: 0.7337 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5282 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00011: val_loss improved from 24.95366 to 24.87683, saving model to test.hdf5\n",
      "Epoch 12/100\n",
      "3396/3396 [==============================] - 1s 301us/step - loss: 36.2555 - aapl_loss: 0.7965 - msft_loss: 0.7643 - aapl_acc: 0.4929 - msft_acc: 0.4920 - val_loss: 24.8240 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5282 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00012: val_loss improved from 24.87683 to 24.82402, saving model to test.hdf5\n",
      "Epoch 13/100\n",
      "3396/3396 [==============================] - 1s 308us/step - loss: 36.2038 - aapl_loss: 0.7936 - msft_loss: 0.7551 - aapl_acc: 0.4932 - msft_acc: 0.5012 - val_loss: 24.7753 - val_aapl_loss: 0.7336 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5282 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00013: val_loss improved from 24.82402 to 24.77531, saving model to test.hdf5\n",
      "Epoch 14/100\n",
      "3396/3396 [==============================] - 1s 381us/step - loss: 36.3160 - aapl_loss: 0.7823 - msft_loss: 0.7682 - aapl_acc: 0.5006 - msft_acc: 0.4853 - val_loss: 24.8137 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7156 - val_aapl_acc: 0.5259 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 24.77531\n",
      "Epoch 15/100\n",
      "3396/3396 [==============================] - 1s 337us/step - loss: 35.9833 - aapl_loss: 0.7835 - msft_loss: 0.7596 - aapl_acc: 0.4994 - msft_acc: 0.5121 - val_loss: 24.7782 - val_aapl_loss: 0.7340 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5294 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 24.77531\n",
      "Epoch 16/100\n",
      "3396/3396 [==============================] - 1s 300us/step - loss: 36.1120 - aapl_loss: 0.7847 - msft_loss: 0.7545 - aapl_acc: 0.5000 - msft_acc: 0.5038 - val_loss: 24.7204 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7153 - val_aapl_acc: 0.5294 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00016: val_loss improved from 24.77531 to 24.72042, saving model to test.hdf5\n",
      "Epoch 17/100\n",
      "3396/3396 [==============================] - 1s 312us/step - loss: 35.8395 - aapl_loss: 0.7778 - msft_loss: 0.7549 - aapl_acc: 0.5103 - msft_acc: 0.5000 - val_loss: 24.6876 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7155 - val_aapl_acc: 0.5306 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00017: val_loss improved from 24.72042 to 24.68760, saving model to test.hdf5\n",
      "Epoch 18/100\n",
      "3396/3396 [==============================] - 1s 322us/step - loss: 35.7676 - aapl_loss: 0.7839 - msft_loss: 0.7664 - aapl_acc: 0.4991 - msft_acc: 0.4915 - val_loss: 24.6586 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7156 - val_aapl_acc: 0.5294 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00018: val_loss improved from 24.68760 to 24.65861, saving model to test.hdf5\n",
      "Epoch 19/100\n",
      "3396/3396 [==============================] - 1s 353us/step - loss: 35.9308 - aapl_loss: 0.7839 - msft_loss: 0.7672 - aapl_acc: 0.5027 - msft_acc: 0.4909 - val_loss: 24.6266 - val_aapl_loss: 0.7337 - val_msft_loss: 0.7152 - val_aapl_acc: 0.5294 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00019: val_loss improved from 24.65861 to 24.62658, saving model to test.hdf5\n",
      "Epoch 20/100\n",
      "3396/3396 [==============================] - 1s 295us/step - loss: 35.8237 - aapl_loss: 0.7876 - msft_loss: 0.7489 - aapl_acc: 0.4965 - msft_acc: 0.5056 - val_loss: 24.5762 - val_aapl_loss: 0.7336 - val_msft_loss: 0.7155 - val_aapl_acc: 0.5235 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00020: val_loss improved from 24.62658 to 24.57620, saving model to test.hdf5\n",
      "Epoch 21/100\n",
      "3396/3396 [==============================] - 1s 296us/step - loss: 35.8044 - aapl_loss: 0.7843 - msft_loss: 0.7541 - aapl_acc: 0.5000 - msft_acc: 0.4979 - val_loss: 24.5430 - val_aapl_loss: 0.7336 - val_msft_loss: 0.7157 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00021: val_loss improved from 24.57620 to 24.54298, saving model to test.hdf5\n",
      "Epoch 22/100\n",
      "3396/3396 [==============================] - 1s 327us/step - loss: 35.6503 - aapl_loss: 0.7903 - msft_loss: 0.7581 - aapl_acc: 0.4956 - msft_acc: 0.4988 - val_loss: 24.4972 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7162 - val_aapl_acc: 0.5247 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00022: val_loss improved from 24.54298 to 24.49720, saving model to test.hdf5\n",
      "Epoch 23/100\n",
      "3396/3396 [==============================] - 1s 376us/step - loss: 35.7522 - aapl_loss: 0.7807 - msft_loss: 0.7577 - aapl_acc: 0.5085 - msft_acc: 0.5029 - val_loss: 24.4505 - val_aapl_loss: 0.7338 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5247 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00023: val_loss improved from 24.49720 to 24.45048, saving model to test.hdf5\n",
      "Epoch 24/100\n",
      "3396/3396 [==============================] - 1s 376us/step - loss: 35.6186 - aapl_loss: 0.7871 - msft_loss: 0.7585 - aapl_acc: 0.4912 - msft_acc: 0.5009 - val_loss: 24.4000 - val_aapl_loss: 0.7337 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5200 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.45048 to 24.39996, saving model to test.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "3396/3396 [==============================] - 1s 363us/step - loss: 35.5076 - aapl_loss: 0.7839 - msft_loss: 0.7568 - aapl_acc: 0.5059 - msft_acc: 0.5056 - val_loss: 24.3481 - val_aapl_loss: 0.7340 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5212 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00025: val_loss improved from 24.39996 to 24.34810, saving model to test.hdf5\n",
      "Epoch 26/100\n",
      "3396/3396 [==============================] - 1s 388us/step - loss: 35.3959 - aapl_loss: 0.7804 - msft_loss: 0.7529 - aapl_acc: 0.5003 - msft_acc: 0.5074 - val_loss: 24.3213 - val_aapl_loss: 0.7341 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.34810 to 24.32128, saving model to test.hdf5\n",
      "Epoch 27/100\n",
      "3396/3396 [==============================] - 1s 297us/step - loss: 35.4626 - aapl_loss: 0.7852 - msft_loss: 0.7568 - aapl_acc: 0.4973 - msft_acc: 0.5027 - val_loss: 24.2396 - val_aapl_loss: 0.7340 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5212 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00027: val_loss improved from 24.32128 to 24.23961, saving model to test.hdf5\n",
      "Epoch 28/100\n",
      "3396/3396 [==============================] - 1s 298us/step - loss: 35.2440 - aapl_loss: 0.7815 - msft_loss: 0.7645 - aapl_acc: 0.5047 - msft_acc: 0.5021 - val_loss: 24.2110 - val_aapl_loss: 0.7341 - val_msft_loss: 0.7158 - val_aapl_acc: 0.5212 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00028: val_loss improved from 24.23961 to 24.21103, saving model to test.hdf5\n",
      "Epoch 29/100\n",
      "3396/3396 [==============================] - 1s 368us/step - loss: 35.2989 - aapl_loss: 0.7997 - msft_loss: 0.7639 - aapl_acc: 0.4926 - msft_acc: 0.4906 - val_loss: 24.1959 - val_aapl_loss: 0.7339 - val_msft_loss: 0.7154 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00029: val_loss improved from 24.21103 to 24.19592, saving model to test.hdf5\n",
      "Epoch 30/100\n",
      "3396/3396 [==============================] - 1s 315us/step - loss: 35.2408 - aapl_loss: 0.7876 - msft_loss: 0.7572 - aapl_acc: 0.4847 - msft_acc: 0.4971 - val_loss: 24.1396 - val_aapl_loss: 0.7339 - val_msft_loss: 0.7156 - val_aapl_acc: 0.5235 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00030: val_loss improved from 24.19592 to 24.13962, saving model to test.hdf5\n",
      "Epoch 31/100\n",
      "3396/3396 [==============================] - 1s 310us/step - loss: 35.2023 - aapl_loss: 0.7830 - msft_loss: 0.7509 - aapl_acc: 0.5053 - msft_acc: 0.5194 - val_loss: 24.0819 - val_aapl_loss: 0.7340 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00031: val_loss improved from 24.13962 to 24.08185, saving model to test.hdf5\n",
      "Epoch 32/100\n",
      "3396/3396 [==============================] - 1s 352us/step - loss: 35.2075 - aapl_loss: 0.7872 - msft_loss: 0.7666 - aapl_acc: 0.5035 - msft_acc: 0.5059 - val_loss: 23.9933 - val_aapl_loss: 0.7343 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00032: val_loss improved from 24.08185 to 23.99329, saving model to test.hdf5\n",
      "Epoch 33/100\n",
      "3396/3396 [==============================] - 1s 309us/step - loss: 35.0763 - aapl_loss: 0.7821 - msft_loss: 0.7579 - aapl_acc: 0.4906 - msft_acc: 0.5053 - val_loss: 23.9318 - val_aapl_loss: 0.7345 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4941\n",
      "\n",
      "Epoch 00033: val_loss improved from 23.99329 to 23.93175, saving model to test.hdf5\n",
      "Epoch 34/100\n",
      "3396/3396 [==============================] - 1s 317us/step - loss: 34.9125 - aapl_loss: 0.7841 - msft_loss: 0.7565 - aapl_acc: 0.4912 - msft_acc: 0.5062 - val_loss: 23.8917 - val_aapl_loss: 0.7348 - val_msft_loss: 0.7165 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00034: val_loss improved from 23.93175 to 23.89172, saving model to test.hdf5\n",
      "Epoch 35/100\n",
      "3396/3396 [==============================] - 1s 322us/step - loss: 34.8572 - aapl_loss: 0.7932 - msft_loss: 0.7599 - aapl_acc: 0.4885 - msft_acc: 0.5038 - val_loss: 23.8595 - val_aapl_loss: 0.7349 - val_msft_loss: 0.7170 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4929\n",
      "\n",
      "Epoch 00035: val_loss improved from 23.89172 to 23.85951, saving model to test.hdf5\n",
      "Epoch 36/100\n",
      "3396/3396 [==============================] - 1s 311us/step - loss: 34.8582 - aapl_loss: 0.7830 - msft_loss: 0.7564 - aapl_acc: 0.5027 - msft_acc: 0.5035 - val_loss: 23.8473 - val_aapl_loss: 0.7349 - val_msft_loss: 0.7165 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4953\n",
      "\n",
      "Epoch 00036: val_loss improved from 23.85951 to 23.84727, saving model to test.hdf5\n",
      "Epoch 37/100\n",
      "3396/3396 [==============================] - 1s 351us/step - loss: 34.9940 - aapl_loss: 0.7858 - msft_loss: 0.7632 - aapl_acc: 0.5021 - msft_acc: 0.4941 - val_loss: 23.7987 - val_aapl_loss: 0.7347 - val_msft_loss: 0.7163 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4965\n",
      "\n",
      "Epoch 00037: val_loss improved from 23.84727 to 23.79868, saving model to test.hdf5\n",
      "Epoch 38/100\n",
      "3396/3396 [==============================] - 1s 321us/step - loss: 34.7554 - aapl_loss: 0.7906 - msft_loss: 0.7539 - aapl_acc: 0.4920 - msft_acc: 0.5065 - val_loss: 23.7868 - val_aapl_loss: 0.7348 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4988\n",
      "\n",
      "Epoch 00038: val_loss improved from 23.79868 to 23.78682, saving model to test.hdf5\n",
      "Epoch 39/100\n",
      "3396/3396 [==============================] - 1s 336us/step - loss: 34.6668 - aapl_loss: 0.7907 - msft_loss: 0.7547 - aapl_acc: 0.4850 - msft_acc: 0.4997 - val_loss: 23.7449 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7159 - val_aapl_acc: 0.5247 - val_msft_acc: 0.4988\n",
      "\n",
      "Epoch 00039: val_loss improved from 23.78682 to 23.74491, saving model to test.hdf5\n",
      "Epoch 40/100\n",
      "3396/3396 [==============================] - 1s 304us/step - loss: 34.6394 - aapl_loss: 0.7864 - msft_loss: 0.7678 - aapl_acc: 0.5000 - msft_acc: 0.4882 - val_loss: 23.7391 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7162 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00040: val_loss improved from 23.74491 to 23.73908, saving model to test.hdf5\n",
      "Epoch 41/100\n",
      "3396/3396 [==============================] - 1s 322us/step - loss: 34.4493 - aapl_loss: 0.7840 - msft_loss: 0.7722 - aapl_acc: 0.5127 - msft_acc: 0.4879 - val_loss: 23.6920 - val_aapl_loss: 0.7350 - val_msft_loss: 0.7163 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00041: val_loss improved from 23.73908 to 23.69200, saving model to test.hdf5\n",
      "Epoch 42/100\n",
      "3396/3396 [==============================] - 1s 280us/step - loss: 34.4408 - aapl_loss: 0.7925 - msft_loss: 0.7571 - aapl_acc: 0.4947 - msft_acc: 0.5053 - val_loss: 23.6072 - val_aapl_loss: 0.7350 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5224 - val_msft_acc: 0.4988\n",
      "\n",
      "Epoch 00042: val_loss improved from 23.69200 to 23.60717, saving model to test.hdf5\n",
      "Epoch 43/100\n",
      "3396/3396 [==============================] - 1s 268us/step - loss: 34.4023 - aapl_loss: 0.7817 - msft_loss: 0.7680 - aapl_acc: 0.5056 - msft_acc: 0.4873 - val_loss: 23.5641 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7167 - val_aapl_acc: 0.5259 - val_msft_acc: 0.4976\n",
      "\n",
      "Epoch 00043: val_loss improved from 23.60717 to 23.56410, saving model to test.hdf5\n",
      "Epoch 44/100\n",
      "3396/3396 [==============================] - 1s 272us/step - loss: 34.3572 - aapl_loss: 0.7853 - msft_loss: 0.7644 - aapl_acc: 0.5044 - msft_acc: 0.4956 - val_loss: 23.5464 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00044: val_loss improved from 23.56410 to 23.54641, saving model to test.hdf5\n",
      "Epoch 45/100\n",
      "3396/3396 [==============================] - 1s 272us/step - loss: 34.1523 - aapl_loss: 0.7820 - msft_loss: 0.7674 - aapl_acc: 0.5065 - msft_acc: 0.4856 - val_loss: 23.5172 - val_aapl_loss: 0.7354 - val_msft_loss: 0.7165 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00045: val_loss improved from 23.54641 to 23.51719, saving model to test.hdf5\n",
      "Epoch 46/100\n",
      "3396/3396 [==============================] - 1s 275us/step - loss: 34.3101 - aapl_loss: 0.7793 - msft_loss: 0.7536 - aapl_acc: 0.5077 - msft_acc: 0.5062 - val_loss: 23.4340 - val_aapl_loss: 0.7350 - val_msft_loss: 0.7169 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00046: val_loss improved from 23.51719 to 23.43400, saving model to test.hdf5\n",
      "Epoch 47/100\n",
      "3396/3396 [==============================] - 1s 270us/step - loss: 34.2730 - aapl_loss: 0.7977 - msft_loss: 0.7671 - aapl_acc: 0.4885 - msft_acc: 0.4870 - val_loss: 23.4010 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7168 - val_aapl_acc: 0.5235 - val_msft_acc: 0.4988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_loss improved from 23.43400 to 23.40098, saving model to test.hdf5\n",
      "Epoch 48/100\n",
      "3396/3396 [==============================] - 1s 268us/step - loss: 34.1760 - aapl_loss: 0.7858 - msft_loss: 0.7501 - aapl_acc: 0.5071 - msft_acc: 0.5127 - val_loss: 23.3245 - val_aapl_loss: 0.7354 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00048: val_loss improved from 23.40098 to 23.32447, saving model to test.hdf5\n",
      "Epoch 49/100\n",
      "3396/3396 [==============================] - 1s 270us/step - loss: 34.1763 - aapl_loss: 0.7885 - msft_loss: 0.7556 - aapl_acc: 0.4832 - msft_acc: 0.5035 - val_loss: 23.2421 - val_aapl_loss: 0.7353 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00049: val_loss improved from 23.32447 to 23.24205, saving model to test.hdf5\n",
      "Epoch 50/100\n",
      "3396/3396 [==============================] - 1s 272us/step - loss: 34.1195 - aapl_loss: 0.7900 - msft_loss: 0.7580 - aapl_acc: 0.5041 - msft_acc: 0.4973 - val_loss: 23.2299 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00050: val_loss improved from 23.24205 to 23.22990, saving model to test.hdf5\n",
      "Epoch 51/100\n",
      "3396/3396 [==============================] - 1s 271us/step - loss: 34.0450 - aapl_loss: 0.7880 - msft_loss: 0.7486 - aapl_acc: 0.5015 - msft_acc: 0.5112 - val_loss: 23.1770 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00051: val_loss improved from 23.22990 to 23.17704, saving model to test.hdf5\n",
      "Epoch 52/100\n",
      "3396/3396 [==============================] - 1s 271us/step - loss: 33.9683 - aapl_loss: 0.7855 - msft_loss: 0.7599 - aapl_acc: 0.4994 - msft_acc: 0.4941 - val_loss: 23.1449 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7165 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00052: val_loss improved from 23.17704 to 23.14488, saving model to test.hdf5\n",
      "Epoch 53/100\n",
      "3396/3396 [==============================] - 1s 272us/step - loss: 33.7834 - aapl_loss: 0.7778 - msft_loss: 0.7571 - aapl_acc: 0.5053 - msft_acc: 0.5088 - val_loss: 23.0731 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00053: val_loss improved from 23.14488 to 23.07306, saving model to test.hdf5\n",
      "Epoch 54/100\n",
      "3396/3396 [==============================] - 1s 269us/step - loss: 33.6416 - aapl_loss: 0.7881 - msft_loss: 0.7551 - aapl_acc: 0.4938 - msft_acc: 0.5059 - val_loss: 23.0613 - val_aapl_loss: 0.7353 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00054: val_loss improved from 23.07306 to 23.06133, saving model to test.hdf5\n",
      "Epoch 55/100\n",
      "3396/3396 [==============================] - 1s 269us/step - loss: 33.5623 - aapl_loss: 0.7760 - msft_loss: 0.7624 - aapl_acc: 0.5065 - msft_acc: 0.4915 - val_loss: 23.0122 - val_aapl_loss: 0.7355 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00055: val_loss improved from 23.06133 to 23.01219, saving model to test.hdf5\n",
      "Epoch 56/100\n",
      "3396/3396 [==============================] - 1s 272us/step - loss: 33.6466 - aapl_loss: 0.7875 - msft_loss: 0.7647 - aapl_acc: 0.4985 - msft_acc: 0.4870 - val_loss: 22.9329 - val_aapl_loss: 0.7355 - val_msft_loss: 0.7166 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00056: val_loss improved from 23.01219 to 22.93286, saving model to test.hdf5\n",
      "Epoch 57/100\n",
      "3396/3396 [==============================] - 1s 267us/step - loss: 33.6157 - aapl_loss: 0.7813 - msft_loss: 0.7682 - aapl_acc: 0.5059 - msft_acc: 0.5015 - val_loss: 22.8994 - val_aapl_loss: 0.7354 - val_msft_loss: 0.7168 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00057: val_loss improved from 22.93286 to 22.89945, saving model to test.hdf5\n",
      "Epoch 58/100\n",
      "3396/3396 [==============================] - 1s 270us/step - loss: 33.5868 - aapl_loss: 0.7866 - msft_loss: 0.7535 - aapl_acc: 0.4929 - msft_acc: 0.5212 - val_loss: 22.8193 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5035\n",
      "\n",
      "Epoch 00058: val_loss improved from 22.89945 to 22.81926, saving model to test.hdf5\n",
      "Epoch 59/100\n",
      "3396/3396 [==============================] - 1s 302us/step - loss: 33.3346 - aapl_loss: 0.7848 - msft_loss: 0.7604 - aapl_acc: 0.5027 - msft_acc: 0.5068 - val_loss: 22.7759 - val_aapl_loss: 0.7356 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00059: val_loss improved from 22.81926 to 22.77594, saving model to test.hdf5\n",
      "Epoch 60/100\n",
      "3396/3396 [==============================] - 1s 323us/step - loss: 33.4375 - aapl_loss: 0.7814 - msft_loss: 0.7549 - aapl_acc: 0.5085 - msft_acc: 0.5009 - val_loss: 22.7279 - val_aapl_loss: 0.7353 - val_msft_loss: 0.7163 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5000\n",
      "\n",
      "Epoch 00060: val_loss improved from 22.77594 to 22.72794, saving model to test.hdf5\n",
      "Epoch 61/100\n",
      "3396/3396 [==============================] - 1s 343us/step - loss: 33.2201 - aapl_loss: 0.7828 - msft_loss: 0.7634 - aapl_acc: 0.4994 - msft_acc: 0.4956 - val_loss: 22.7432 - val_aapl_loss: 0.7354 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.72794\n",
      "Epoch 62/100\n",
      "3396/3396 [==============================] - 1s 336us/step - loss: 33.1811 - aapl_loss: 0.7809 - msft_loss: 0.7570 - aapl_acc: 0.5021 - msft_acc: 0.5038 - val_loss: 22.7204 - val_aapl_loss: 0.7354 - val_msft_loss: 0.7166 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00062: val_loss improved from 22.72794 to 22.72042, saving model to test.hdf5\n",
      "Epoch 63/100\n",
      "3396/3396 [==============================] - 1s 322us/step - loss: 33.2094 - aapl_loss: 0.7816 - msft_loss: 0.7649 - aapl_acc: 0.5074 - msft_acc: 0.4950 - val_loss: 22.6671 - val_aapl_loss: 0.7353 - val_msft_loss: 0.7162 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5035\n",
      "\n",
      "Epoch 00063: val_loss improved from 22.72042 to 22.66709, saving model to test.hdf5\n",
      "Epoch 64/100\n",
      "3396/3396 [==============================] - 1s 366us/step - loss: 33.0723 - aapl_loss: 0.7912 - msft_loss: 0.7603 - aapl_acc: 0.4968 - msft_acc: 0.5062 - val_loss: 22.6230 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00064: val_loss improved from 22.66709 to 22.62296, saving model to test.hdf5\n",
      "Epoch 65/100\n",
      "3396/3396 [==============================] - 1s 331us/step - loss: 33.0978 - aapl_loss: 0.7870 - msft_loss: 0.7517 - aapl_acc: 0.4962 - msft_acc: 0.5074 - val_loss: 22.5613 - val_aapl_loss: 0.7354 - val_msft_loss: 0.7165 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00065: val_loss improved from 22.62296 to 22.56129, saving model to test.hdf5\n",
      "Epoch 66/100\n",
      "3396/3396 [==============================] - 1s 330us/step - loss: 33.1034 - aapl_loss: 0.7885 - msft_loss: 0.7651 - aapl_acc: 0.5047 - msft_acc: 0.5003 - val_loss: 22.4957 - val_aapl_loss: 0.7355 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5247 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00066: val_loss improved from 22.56129 to 22.49570, saving model to test.hdf5\n",
      "Epoch 67/100\n",
      "3396/3396 [==============================] - 1s 320us/step - loss: 33.0372 - aapl_loss: 0.7932 - msft_loss: 0.7590 - aapl_acc: 0.4926 - msft_acc: 0.5021 - val_loss: 22.4519 - val_aapl_loss: 0.7355 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00067: val_loss improved from 22.49570 to 22.45191, saving model to test.hdf5\n",
      "Epoch 68/100\n",
      "3396/3396 [==============================] - 1s 326us/step - loss: 32.8219 - aapl_loss: 0.7830 - msft_loss: 0.7656 - aapl_acc: 0.5029 - msft_acc: 0.4918 - val_loss: 22.3964 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00068: val_loss improved from 22.45191 to 22.39637, saving model to test.hdf5\n",
      "Epoch 69/100\n",
      "3396/3396 [==============================] - 1s 320us/step - loss: 32.7512 - aapl_loss: 0.7869 - msft_loss: 0.7719 - aapl_acc: 0.4956 - msft_acc: 0.4879 - val_loss: 22.3374 - val_aapl_loss: 0.7350 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00069: val_loss improved from 22.39637 to 22.33742, saving model to test.hdf5\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3396/3396 [==============================] - 1s 310us/step - loss: 32.7878 - aapl_loss: 0.7789 - msft_loss: 0.7669 - aapl_acc: 0.5018 - msft_acc: 0.4909 - val_loss: 22.3108 - val_aapl_loss: 0.7350 - val_msft_loss: 0.7162 - val_aapl_acc: 0.5224 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00070: val_loss improved from 22.33742 to 22.31079, saving model to test.hdf5\n",
      "Epoch 71/100\n",
      "3396/3396 [==============================] - 1s 300us/step - loss: 32.6024 - aapl_loss: 0.7856 - msft_loss: 0.7577 - aapl_acc: 0.4979 - msft_acc: 0.5003 - val_loss: 22.2435 - val_aapl_loss: 0.7348 - val_msft_loss: 0.7165 - val_aapl_acc: 0.5212 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00071: val_loss improved from 22.31079 to 22.24346, saving model to test.hdf5\n",
      "Epoch 72/100\n",
      "3396/3396 [==============================] - 1s 310us/step - loss: 32.5872 - aapl_loss: 0.7943 - msft_loss: 0.7623 - aapl_acc: 0.4903 - msft_acc: 0.5003 - val_loss: 22.2263 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7162 - val_aapl_acc: 0.5212 - val_msft_acc: 0.5012\n",
      "\n",
      "Epoch 00072: val_loss improved from 22.24346 to 22.22625, saving model to test.hdf5\n",
      "Epoch 73/100\n",
      "3396/3396 [==============================] - 1s 317us/step - loss: 32.5014 - aapl_loss: 0.7893 - msft_loss: 0.7582 - aapl_acc: 0.4985 - msft_acc: 0.4947 - val_loss: 22.1601 - val_aapl_loss: 0.7349 - val_msft_loss: 0.7161 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5024\n",
      "\n",
      "Epoch 00073: val_loss improved from 22.22625 to 22.16007, saving model to test.hdf5\n",
      "Epoch 74/100\n",
      "3396/3396 [==============================] - 1s 354us/step - loss: 32.4933 - aapl_loss: 0.7896 - msft_loss: 0.7561 - aapl_acc: 0.4894 - msft_acc: 0.5056 - val_loss: 22.1015 - val_aapl_loss: 0.7350 - val_msft_loss: 0.7160 - val_aapl_acc: 0.5235 - val_msft_acc: 0.5035\n",
      "\n",
      "Epoch 00074: val_loss improved from 22.16007 to 22.10154, saving model to test.hdf5\n",
      "Epoch 75/100\n",
      "3396/3396 [==============================] - 1s 303us/step - loss: 32.3809 - aapl_loss: 0.7909 - msft_loss: 0.7629 - aapl_acc: 0.5038 - msft_acc: 0.4965 - val_loss: 22.0679 - val_aapl_loss: 0.7352 - val_msft_loss: 0.7166 - val_aapl_acc: 0.5259 - val_msft_acc: 0.5035\n",
      "\n",
      "Epoch 00075: val_loss improved from 22.10154 to 22.06789, saving model to test.hdf5\n",
      "Epoch 76/100\n",
      "3396/3396 [==============================] - 1s 320us/step - loss: 32.3925 - aapl_loss: 0.7816 - msft_loss: 0.7522 - aapl_acc: 0.5121 - msft_acc: 0.5053 - val_loss: 22.0057 - val_aapl_loss: 0.7351 - val_msft_loss: 0.7164 - val_aapl_acc: 0.5259 - val_msft_acc: 0.5035\n",
      "\n",
      "Epoch 00076: val_loss improved from 22.06789 to 22.00567, saving model to test.hdf5\n",
      "Epoch 77/100\n",
      "3200/3396 [===========================>..] - ETA: 0s - loss: 32.5607 - aapl_loss: 0.7789 - msft_loss: 0.7649 - aapl_acc: 0.5153 - msft_acc: 0.4938"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=25, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"test.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "model = create_model(company_names)\n",
    "history = model.fit(X_train, y_train, \n",
    "          epochs = 100, \n",
    "          batch_size = 128, \n",
    "          verbose=1,\n",
    "          validation_split=0.20,\n",
    "          callbacks=[reduce_lr, checkpointer],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_train(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "explain_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать на высокочастотных данных (каждый час, каждые пять минут) — больше данных — больше паттернов — меньше переобучения\n",
    "Использовать более продвинутые архитектуры нейронных сетей, которые предназначены для работы с последовательностями — convolutional neural networks, recurrent neural networks\n",
    "Использовать не только цену закрытия, а все данные из нашего .csv (high, low, open, close, volume) — то есть в каждый момент времени обращать внимание на всю доступную информацию\n",
    "Оптимизировать гиперпараметры — размер окна, количество нейронов в скрытых слоях, шаг обучения — все эти параметры были взяты несколько наугад, с помощью случайного поиска можно выяснить, что, возможно, нам надо смотреть на 45 дней назад и учить с меньшим шагом более глубокую сетку.\n",
    "Использовать более подходящие для нашей задачи функции потерь (например, для прогнозирования изменения цены мы могли бы штрафовать нейронную за неправильный знак, обычная MSE к знаку числа инвариантна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://habr.com/ru/company/wunderfund/blog/331310/\n",
    "# https://www.programcreek.com/python/example/93295/keras.layers.convolutional.Convolution1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
